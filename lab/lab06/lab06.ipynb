{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab06.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 06: Modeling, Loss Functions, and Summary Statistics\n",
    "## Due Wednesday, July 10th, 11:59 PM PT\n",
    "\n",
    "\n",
    "In this lab, you will perform modeling on a dataset containing restaurant tips and explore loss functions and summary statistics in the process.\n",
    "\n",
    "To receive credit for a lab, answer all questions correctly and submit before the deadline.\n",
    "\n",
    "**The on-time deadline is Wednesday, July 10th, 11:59 PM PT**. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted. While course staff is happy to help you if you encounter difficulties with submission, we may not be able to respond to late-night requests for assistance (TAs need to sleep, after all!). **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to contact staff for submission support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walkthrough\n",
    "In addition to the lab notebook, we have also released a prerecorded walkthrough video of the lab. This playlist includes a walkthrough for every question in the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "\n",
    "**Note:** These videos were recorded in Spring 2023. There may be slight inconsistencies between the version you are viewing and the version used in the recording, but the content is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"v=pSBM0W860DQ\", list='PLQCcNQgUcDfo1mnau9OkAYF0jwXOvxoGq', istType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk to others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Debugging Guide\n",
    "\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Gradescope, and common `pandas` and RegEx errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Predicting Restaurant Tips\n",
    "\n",
    "In this lab, you will try to predict restaurant tips from a set of data in several ways:\n",
    "\n",
    "**Question 1:** Given one piece of information—the total bill $x$—**use a linear model with $L_2$ loss** to predict the tip $\\hat{y}$ as a linear function of $x$. \n",
    "\n",
    "**Question 2:** Given one piece of information—the total bill $x$—**use a linear model with $L_1$ loss** to predict the tip $\\hat{y}$ as a linear function of $x$.\n",
    "\n",
    "**Question 3:** Assume no information is given, use a **constant model $\\hat{y} = \\theta_0$ with $L_2$ loss** to predict the tip $\\hat{y}$ as a summary statistic, $\\theta_0$.\n",
    "\n",
    "<br/>\n",
    "\n",
    "First, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data; no further action is needed.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Quick EDA/Subplots Demo\n",
    "\n",
    "The below plot graphs the distribution of tips in this dataset, both in **absolute amounts ($)** and as a **fraction of the total bill** (post-tax, but pre-tip). `plt.subplots` below helps us plot two graphs side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate the plot; no further action needed.\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "sns.histplot(tips['tip'], bins=20, stat=\"proportion\", ax=ax[0])\n",
    "sns.histplot(tips['tip']/tips['total_bill'], bins=20, stat=\"proportion\", ax=ax[1])\n",
    "ax[0].set_xlabel(\"Amount ($)\")\n",
    "ax[1].set_xlabel(\"Fraction of total bill\")\n",
    "ax[0].set_ylim((0, 0.35))\n",
    "ax[1].set_ylim((0, 0.35))\n",
    "ax[1].set_ylabel(\"\") # for cleaner visualization\n",
    "fig.suptitle(\"Restaurant Tips\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll estimate the tip in **absolute amounts ($)**. The above plot is just to confirm your expectations about the `tips` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 1: Tips as a Linear Function of Total Bill with $L_2$ Loss\n",
    "\n",
    "In this section, you will follow the modeling process above and use `total_bill` to compute the predicted tips. First, we identify the values we already have.\n",
    "\n",
    "`y_tips`: Each actual tip in our dataset is $y$, which is what we call the **observed value** or **dependent variable**. We want to predict each observed value as $\\hat{y}$. We'll save the observed tip values in this **NumPy array**.\n",
    "\n",
    "`x_total_bills`: Each observed total bill in our dataset is $x$, which is what we call the **feature**, **predictor**, or **independent variable**. We want to predict each observed value using $x$. We'll save the observed total bill values (post-tax but pre-tip) in this **NumPy array**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define x and y array; no further action needed.\n",
    "y_tips = np.array(tips['tip'])               # Array of observed tips\n",
    "x_total_bills = np.array(tips['total_bill']) # Array of total bill amounts\n",
    "\n",
    "print(\"total bills\", x_total_bills.shape)\n",
    "print(\"tips\", y_tips.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the steps of the modeling process covered in the lecture:\n",
    "\n",
    "A. Define a **model.**\n",
    "\n",
    "B. Choose a **loss function** and calculate the **average loss** on our dataset.\n",
    "\n",
    "C. Find the best value of $\\theta$, known as $\\hat{\\theta}$, that **minimizes** loss. There can be multiple such $\\theta$ values.\n",
    "\n",
    "D. Evaluate the model performance (not covered in this lab). \n",
    "\n",
    "We'll go through the first three steps of this process next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## A: Define a model.\n",
    "\n",
    "We will define our model as the **linear model** that takes a single input feature, `total_bill`, $x$, and predicts the dependent variable, $\\hat{y}$:\n",
    "\n",
    "$$\\large\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "\n",
    "$\\theta_0$ and $\\theta_1$ are what we call **parameters**. Our modeling goal is to find the value of our parameter(s) that **best fit our data**.\n",
    "\n",
    "We have a choice over which $\\theta_0$ and $\\theta_1$ we pick (using the data at hand), but ultimately we can only pick one to report, so we want to find the optimal parameter(s) $\\hat{\\theta_0}$ and $\\hat{\\theta_1}$.\n",
    "\n",
    "Our modeling task is then to pick the best values $\\theta_0 = \\hat{\\theta}_0$ and $\\theta_1 = \\hat{\\theta_1}$ from our data. \n",
    "\n",
    "Then, given the total bill $x$, we can predict the tip as $\\hat{y} = \\hat{\\theta}_0 + \\hat{\\theta}_1 x$.\n",
    "\n",
    "No code to write here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## B: Define a loss function.\n",
    "\n",
    "Next, in order to pick our $\\theta_0$ and $\\theta_1$, we need to define a **loss function**, which is a measure of how well a model is able to predict the expected outcome. In other words, it measures the deviation of the observed value $y$ from the predicted value $\\hat{y}$.\n",
    "\n",
    "We will use **squared loss** (also known as the $L_2$ loss, pronounced \"ell-two\"). For an observed tip value $y$ (i.e., the real tip), our prediction of the tip $\\hat{y}$ would give an $L_2$ loss of:\n",
    "\n",
    "$$\\large L_2(y, \\hat{y}) = \\large (y - \\hat{y})^2 = \\large (y - (\\theta_0 + \\theta_1 x))^2 $$\n",
    "\n",
    "\n",
    "We just defined loss for a single data point. Let's extend the above function to our entire dataset by taking the **average loss** across the dataset.\n",
    "\n",
    "Let the dataset $\\mathcal{D}$ be the set of observations: $\\mathcal{D} = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$, where $(x_i, y_i)$ are the $i^{th}$ total bill and tip, respectively, in our dataset.\n",
    "\n",
    "We can define the average loss over the dataset using squared loss (also known as **Mean Squared Error (MSE)**) as:\n",
    "\n",
    "$$\\large R(\\theta_0, \\theta_1) = \\large \\frac{1}{n} \\sum_{i=1}^n L_2(y_i, \\hat{y}_i) $$\n",
    "$$= \\large \\frac{1}{n} \\sum_{i = 1}^n(y_i - (\\theta_0 + \\theta_1 x_i))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "Define the `mse_tips_linear` function which computes $R(\\theta_0, \\theta_1)$ as the **Mean Squared Error (MSE)** on the tips data for a linear model with parameters $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "**Hint:**\n",
    "* This function takes in two parameters `theta0` and `theta1`.\n",
    "* You should use the `NumPy` arrays `x_total_bills` and `y_tips` defined at the beginning of Question 1.\n",
    "* We've included some skeleton code, but feel free to write your own as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_tips_linear(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Calculate the mean square error on the tips data for a linear model.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    theta0 : intercept of the fitted linear model\n",
    "    theta1 : slope of the fitted linear model\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The mean square error on the tips data for a linear model.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hats = ...\n",
    "    ...\n",
    "\n",
    "mse_tips_linear(0.9, 0.1) # Arbitrarily pick theta0 = 0.9, theta1 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Find the $\\theta_0$ and $\\theta_1$ that minimize loss.\n",
    "\n",
    "Now we can go about choosing our \"best\" value of $\\vec{\\theta} = \\begin{bmatrix}\\theta_0\\\\ \\theta_1 \\end{bmatrix}$, \n",
    "which we call $\\hat{\\theta}$, that minimizes our MSE. There are several approaches to computing $\\hat{\\theta}$ that we'll explore in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analytical Solution\n",
    "\n",
    "In lecture, we derived the following optimal parameters $\\hat{\\theta}_1$ and $\\hat{\\theta}_0$:\n",
    "\n",
    "$$\\large \\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "\n",
    "$$\\large \\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1\\bar{x}$$\n",
    "\n",
    "and the prediction of the tip of the $i^{th}$ bill, $\\hat{y_i}$, will be:\n",
    "$$\\large \\hat{y}_i = \\hat{\\theta}_0 + \\hat{\\theta}_1x_i$$\n",
    "\n",
    "where $\\bar{x}$, $\\bar{y}$, $\\sigma_x$, $\\sigma_y$ correspond to the means and standard deviations of $x$ and $y$, respectively, and $r$ is the correlation coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1b\n",
    "\n",
    "Assign `x_bar`, `y_bar`, `std_x`, `std_y`, and `r` to be the means, standard deviations, and correlation coefficient for our dataset.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* Remember, in our case, `y` is `y_tips`, and `x` is `x_total_bills`.\n",
    "* You may find `np.corrcoef` [(documentation)](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html) handy in computing `r`. Note that the output of `np.corrcoef` is a matrix, not a number, so you'll need to collect the correlation coefficient by indexing in the returned matrix.\n",
    "* You may find `np.std` [(documentation)](https://numpy.org/doc/stable/reference/generated/numpy.std.html) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_bar = ...\n",
    "y_bar = ...\n",
    "std_x = ...\n",
    "std_y = ...\n",
    "r = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now, set `theta0_hat` and `theta1_hat` correctly, in terms of the variables you defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta1_hat = ...\n",
    "theta0_hat = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1d\n",
    "\n",
    "Now, use `theta0_hat` and `theta1_hat` to implement the `predict_tip_linear` function, which predicts the tip for a given total bill amount of `bill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_tip_linear(bill):\n",
    "    ...\n",
    "\n",
    "# Do not edit below this line.\n",
    "bill = 20\n",
    "print(f\"\"\"If you have a ${bill} bill, Part B's modeling process\n",
    "    predicts that you will pay a tip of ${predict_tip_linear(bill):.2f}.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Numerical solution\n",
    "\n",
    "Suppose we didn't have a closed-form solution to finding our optimal $\\hat{\\theta}$ that minimizes loss.\n",
    "`scipy.optimize.minimize` is a powerful **numerical method** that can determine the optimal value of a variety of different functions. In practice, it is used to minimize functions that either lack analytical solutions or have solutions that are difficult to obtain. \n",
    "\n",
    "It may be overkill for our simple example, but nonetheless, we will show you how to use it, as it will become useful in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial: `scipy.optimize.minimize`\n",
    "\n",
    "The cell below plots some arbitrary 4th degree polynomial function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot a 4th degree polynomial; no further action is needed.\n",
    "x_values = np.linspace(-4, 2.5, 100)\n",
    "\n",
    "def fx(x):\n",
    "    return 0.1 * x**4 + 0.2*x**3 + 0.2 * x **2 + 1 * x + 10\n",
    "\n",
    "plt.plot(x_values, fx(x_values));\n",
    "plt.title(\"Arbitrary 4th degree polynomial\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the plot, we see that the x that minimizes the function is slightly larger than -2. What if we want the exact value? We will demonstrate how to grab the minimum value and the optimal `x` in the following cell.\n",
    "\n",
    "The function `minimize` from [`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) will attempt to minimize any function you throw at it. Try running the cell below, and you will see that `minimize` seems to get the answer correct.\n",
    "\n",
    "Note: For today, we'll let `minimize` work as if by magic. We'll discuss how `minimize` works later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to minimize fx; no further action is required.\n",
    "from scipy.optimize import minimize\n",
    "minimize(fx, x0 = 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "\n",
    "1. `fun`: The minimum value of the function. <br/>\n",
    "2. `x`: The `x` which minimizes the function. We can index into the object returned by `minimize` to get these values. We have to add the additional `[0]` at the end because the minimizing `x` is returned as an array, but this is not necessarily the case for other attributes (i.e. `fun`), shown in the cell below. This means that `minimize` can also minimize multivariable functions, which we'll see in the second half of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to find the minimum and minimizer of fx; no further action is required.\n",
    "min_result = minimize(fx, x0 = 1.1)\n",
    "min_of_fx = min_result['fun']\n",
    "x_which_minimizes_fx = min_result['x'][0]\n",
    "min_of_fx, x_which_minimizes_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial guess**: The parameter `x0` that we passed to the `minimize` function is where the `minimize` function starts looking as it tries to find the minimum. For example, above, `minimize` started its search at $x = 1.1$ because that's where we told it to start. For the function above, it doesn't really matter what x we start at because the function is nice and has only a single local minimum. More technically, the function is nice because it is [convex](https://en.wikipedia.org/wiki/Convex_function), a property of functions that we will discuss in a later homework.\n",
    "\n",
    "**Local minima**: `minimize` isn't perfect. For example, if we give it a function with many valleys (also known as local minima) it can get stuck. For example, consider the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot fw; no further action is needed.\n",
    "w_values = np.linspace(-2, 10, 100)\n",
    "\n",
    "def fw(w):\n",
    "    return 0.1 * w**4 - 1.5*w**3 + 6 * w **2 - 1 * w + 10\n",
    "\n",
    "plt.plot(w_values, fw(w_values));\n",
    "plt.title(\"Arbitrary function with local minima\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start the minimization at $w = 6.5$, we'll get stuck in the local minimum at $w = 7.03$. Note that no matter what your actual variable is called in your function (`w` in this case), the `minimize` routine still expects a starting point parameter called `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to minimize fw starting at 6.5; no further action is needed.\n",
    "minimize(fw, x0 = 6.5)    # Initial w = 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to minimize fw starting at 2; no further action is needed.\n",
    "minimize(fw, x0 = 2)    # Initial w = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `minimize` function can minimize functions of multiple variables (useful for numerically computing $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$. There's one quirk, however, which is that the function has to accept its parameters as a single list, so we will define $\\vec{\\theta} = \\begin{bmatrix}\\theta_0\\\\ \\theta_1 \\end{bmatrix}$, as single list input to the function.\n",
    "\n",
    "For example, consider the multivariate $f(u, v) = u^2 - 2 u v - 3 v + 2 v^2$. It turns out this function's minimum is at $(1.5, 1.5)$. To minimize this function, we create `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to minimize f; no further action is needed.\n",
    "def f(theta):\n",
    "    u = theta[0]\n",
    "    v = theta[1]\n",
    "    return u**2 - 2 * u * v - 3 * v + 2 * v**2\n",
    "\n",
    "minimize(f, x0 = [0.0, 0.0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now, let's use this approach to find a numerical solution to the optimal $\\hat{\\theta}$ that minimizes our MSE loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Implement the `mse_tips_linear_list` function, which is exactly like `mse_tips_linear` defined previously except that it takes in a single list of 2 variables rather than two separate variables. For example, `mse_tips_linear_list([2, 3])` should return the same value as `mse_tips_linear(2, 3)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_tips_linear_list(theta):\n",
    "    \"\"\"\n",
    "    Calculate the mean square error on the tips data for a linear model.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    theta : a list containing [theta0, theta1]\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The mean square error on the tips data for a linear model.\n",
    "    \"\"\"\n",
    "        \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1f\n",
    "\n",
    "Now, set `min_scipy_linear` to the result of calling `minimize` to optimize the loss function you just implemented.\n",
    "\n",
    "**Hint:** For autograding purposes, pass in the parameter `x0 = [0.0, 0.0]` to `minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call minimize with initial x0 = [0.0, 0.0]\n",
    "min_scipy_linear = ...\n",
    "min_scipy_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above output from your call to `minimize`, running the following cell will set and print the values of `theta0_hat_scipy` and `theta1_hat_scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define theta0_hat_scipy and theta1_hat_scipy; no further action is needed.\n",
    "theta0_hat_scipy = min_scipy_linear['x'][0]\n",
    "theta1_hat_scipy = min_scipy_linear['x'][1]\n",
    "theta0_hat_scipy, theta1_hat_scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "The following cell will print out the values of `theta0_hat` and `theta1_hat` computed from both methods (\"analytical\" refers to the analytical solution in Question 1c; \"numerical\" refers to the numerical solution in Question 1f). If you've done everything correctly, these should be very close to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to display your result.\n",
    "print('theta0_hat_numerical: ', theta0_hat_scipy)\n",
    "print('theta0_hat_analytical: ', theta0_hat)\n",
    "print('\\n')\n",
    "print('theta1_hat_numerical: ', theta1_hat_scipy)\n",
    "print('theta1_hat_analytical: ', theta1_hat)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Solution\n",
    "\n",
    "Feel free to look at the below 3D plot from different perspectives by changing the values of `elevation` and `azimuth` and verify that the $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$ you computed using either method above minimize the MSE. In the cell below, we plot the MSE for different parameter values. Now that we have two parameters, we have a 3D MSE surface plot (two dimensions for two parameters and one dimension for MSE value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate the plot; no further action is needed.\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Feel free to play around with these values to get different perspectives of the 3D Plot\n",
    "elevation = 30 # Try out: 0, 90\n",
    "azimuth = 20 # Try out: 0, 45\n",
    "\n",
    "theta0_values = np.linspace(-1, 1, 80)\n",
    "theta1_values = np.linspace(-1, 1, 80)\n",
    "\n",
    "mse_values = np.array([[mse_tips_linear(x,y) for x in theta0_values] for y in theta1_values])\n",
    "\n",
    "# Plot \n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(theta0_values, theta1_values)\n",
    "surf = ax.plot_surface(X, Y, mse_values, cmap='viridis')\n",
    "\n",
    "ax.set_title('MSE for different $\\\\theta_0, \\\\theta_1$')\n",
    "ax.set_xlabel('$\\\\theta_0$')\n",
    "ax.set_ylabel('$\\\\theta_1$') \n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "ax.view_init(elev = elevation, azim = azimuth)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Reflecting on the lab so far, we used a 3-step approach to find the \"best\" linear fit:\n",
    "\n",
    "A. Define the linear model $\\hat{y}=\\theta_0 + \\theta_1 x$.\n",
    "\n",
    "B. Define \"best\": Define loss per datapoint ($L_2$ loss) and consequently define loss $R(\\theta_0, \\theta_1)$ over a given dataset as the Mean Squared Error (MSE).\n",
    "\n",
    "C. Find the $\\hat{\\theta}_0$, $\\hat{\\theta}_1$ that minimizes the MSE $R(\\theta_0, \\theta_1)$ in several ways:\n",
    "\n",
    "* **Analytically**: Use calculus to find $\\hat{\\theta}_0$, $\\hat{\\theta}_1$ that minimizes MSE $R(\\theta_0, \\theta_1)$. \n",
    "\n",
    "* **Numerically**: Create a function that returns $R([\\theta_0, \\theta_1])$, the MSE for the given data array for a given $[\\theta_0, \\theta_1]$, and use the scipy `minimize` function to find the minimizing $\\hat{\\theta}_0$, $\\hat{\\theta}_1$.\n",
    "\n",
    "\n",
    "* **Visually**: Create a plot of $R(\\theta_0, \\theta_1)$ vs. $\\theta_0$ and $\\theta_1$, and eyeball the minimizing $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 2: Tips as a Linear Function of Total Bill with $L_1$ Loss\n",
    "\n",
    "In this (short) section, we'll consider how the optimal parameters for the **linear model** would change if we used a different loss function.\n",
    "\n",
    "\n",
    "We will now use **absolute loss** (also known as the $L_1$ loss, pronounced \"ell-one\"). For an observed tip value $y$ (i.e., the real tip), our prediction of the tip $\\hat{y}$ would give an $L_1$ loss of:\n",
    "\n",
    "$$\\large L_1(y, \\hat{y}) = |y - \\hat{y}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "We still define the model's **loss** as **average loss** over the dataset. However, since we now use $L_1$ loss per datapoint in our dataset $\\mathcal{D} = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$, our model's loss is now known as **Mean Absolute Error** (MAE).\n",
    "\n",
    "For the linear model $\\hat{y} = \\theta_0 + \\theta_1 x$:\n",
    "\n",
    "\n",
    "$$\\large R\\left(\\theta_0,\\theta_1\\right) = \\large \\frac{1}{n} \\sum_{i=1}^n L_1(y_i, \\hat{y}_i) $$\n",
    "$$ = \\large \\frac{1}{n} \\sum_{i=1}^n |y_i - (\\theta_0 + \\theta_1 x)| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "Define the `mae_tips_linear` function which computes $R(\\theta_0, \\theta_1)$ as the **Mean Absolute Error (MAE)** on the tips data for a linear model with parameters $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "**Hint**: \n",
    "* You should use the `NumPy` arrays `x_total_bills` and `y_tips` defined at the beginning of Question 1.\n",
    "* To most efficiently compute the MAE, try to use `NumPy` functions or expressions that work for each *term* in the loss function name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae_tips_linear(theta0, theta1):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Error on the tips data for a linear model.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    theta0 : intercept of the fitted linear model\n",
    "    theta1 : slope of the fitted linear model\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The mean absolute error on the tips data for a linear model.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hats = ...\n",
    "    ...\n",
    "\n",
    "mae_tips_linear(5.3, 2) # Arbitrarily pick a = 5.3, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "## Numerical Solution\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "There is no simple analytical solution but we can still solve the problem numerically and visually.\n",
    "\n",
    "Set `min_scipy_linear_mae` to the result of calling minimize to optimize the loss function. \n",
    "\n",
    "**Hint:** You may need first to define a helper function or a lambda function that uses the `mae_tips_linear` function you defined earlier.\n",
    "\n",
    "**Note:** SciPy may give you a message similar to `Desired error not necessarily achieved due to precision loss.`, and/or a `success` status of `False`. Don't be alarmed if you see this warning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call minimize with initial x0 = [0.0, 0.0]\n",
    "min_scipy_linear_mae = ...\n",
    "min_scipy_linear_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above output from your call to `minimize`, running the following cell will set and print the values of `theta0_hat_scipy_mae` and `theta1_hat_scipy_mae`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define theta0_hat_scipy_mae and theta1_hat_scipy_mae, no further action needed.\n",
    "theta0_hat_scipy_mae = min_scipy_linear_mae['x'][0]\n",
    "theta1_hat_scipy_mae = min_scipy_linear_mae['x'][1]\n",
    "theta0_hat_scipy_mae, theta1_hat_scipy_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Visual Solution\n",
    "\n",
    "Similar to earlier, run the following code to produce the visualization. Is the numerical solution close to what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate the plot; no further action is needed.\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Feel free to play around with these values to get different perspectives of the 3D Plot\n",
    "elevation = 30 # Try out: 0, 30, 90\n",
    "azimuth = 30 # Try out: 0, 45\n",
    "\n",
    "theta0_values = np.linspace(-1, 1, 80)\n",
    "theta1_values = np.linspace(-1, 1, 80)\n",
    "\n",
    "mae_values = np.array([[mae_tips_linear(x,y) for x in theta0_values] for y in theta1_values])\n",
    "\n",
    "# Plot \n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "X, Y = np.meshgrid(theta0_values, theta1_values)\n",
    "surf = ax.plot_surface(X, Y, mae_values, cmap='viridis')\n",
    "\n",
    "ax.set_title('MAE for different $\\\\theta_0, \\\\theta_1$')\n",
    "ax.set_xlabel('$\\\\theta_0$')\n",
    "ax.set_ylabel('$\\\\theta_1$') \n",
    "ax.set_zlabel('MAE')\n",
    "\n",
    "ax.view_init(elev = elevation, azim = azimuth)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Compare MAE and MSE\n",
    "\n",
    "---\n",
    "\n",
    "### (Ungraded) Question 2e: Thought Question\n",
    "\n",
    "\n",
    "Try to identify any key differences you observe between the MSE and MAE plots and write them down below. This might be more fun with a partner. Note that your answer will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Question 3: Tips as a Constant Function of Total Bill with $L_2$ Loss\n",
    "\n",
    "In this section, you will follow the exact same modeling process but using a different model, the **constant model**. \n",
    "\n",
    "Let us predict any restaurant tip using one single constant value. In other words, let's try to find the best statistic $\\hat{\\theta_0}$ to represent (i.e., **summarize**) the tips from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: Define the model\n",
    "\n",
    "We will define our model as the **constant model**:\n",
    "\n",
    "$$\\large\n",
    "\\hat{y} = \\theta_0\n",
    "$$\n",
    "\n",
    "In other words, regardless of any other details (i.e., features) about their meal, we will always predict our tip $\\hat{y}$ as one single value: $\\theta_0$.\n",
    "\n",
    "It ignores any relationships between variables:\n",
    "\n",
    "- For example, tips likely depend on the bill itself, time of day, how the customers feel, etc.;\n",
    "- Ignoring these factors is a **simplifying assumption**.\n",
    "\n",
    "\n",
    "We call the constant model a **summary statistic**, as we are determining one number that best \"summarizes\" a set of values.\n",
    "\n",
    "\n",
    "No code to write here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B: Define the loss function and loss\n",
    "\n",
    "Next, we'll define our loss function $L(y, \\hat{y})$ and consequently our loss function $R(\\theta_0)$.\n",
    "\n",
    "Similar to our approach to Question 1, we'll use $L_2$ Loss and Mean Squared Error. Let the dataset $\\mathcal{D}$ be the set of observations: $\\mathcal{D} = \\{(y_1), \\ldots, (y_n)\\}$, where $y_i$ are the $i^{th}$ and tip in our dataset.\n",
    "\n",
    "Our $L_2$ Loss and Mean Squared Error are therefore:\n",
    "\n",
    "$$\\large L_2(y, \\hat{y}) = \\large (y - \\hat{y})^2 = \\large (y - \\theta_0)^2 $$\n",
    "\n",
    "$$\\large R(\\theta_0) = \\large \\frac{1}{n} \\sum_{i=1}^n L_2(y_i, \\hat{y}_i) = \\large \\frac{1}{n} \\sum_{i = 1}^n(y_i - \\theta_0)^2\n",
    "$$\n",
    "\n",
    "Notice that because our model is now the constant model $\\hat{y} = \\theta_0$, our final expressions for Loss and MSE are different from Question 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "Define the `mse_tips_constant` function which computes $R(\\theta_0)$ as the **Mean Squared Error** on the tips data for a constant model with parameter $\\theta_0$.\n",
    "\n",
    "\n",
    "**Hints:** \n",
    "* You should use the `NumPy` arrays `y_tips` defined at the beginning of Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mse_tips_constant(theta0):\n",
    "    \"\"\"\n",
    "    Calculate the mean square error on the tips data for a constant model.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    theta0 : fitted constant model\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    The mean square error on the tips data for a constant model.\n",
    "    \"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "mse_tips_constant(5.3) # Arbitrarily pick a = 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Find the $\\theta_0$ that minimizes loss\n",
    "\n",
    "Similar to before, we'd like to try out different approaches to finding the optimal parameter $\\hat{\\theta_0}$ that minimizes MSE.\n",
    "\n",
    "## Visual Solution\n",
    "\n",
    "In the cell below, we plot the MSE for different values of $\\theta_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot MSE; no further action is needed.\n",
    "theta0_values = np.linspace(0, 6, 100)\n",
    "mse = [mse_tips_constant(theta) for theta in theta0_values]\n",
    "plt.plot(theta0_values, mse)\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel('average $L_2$ loss')\n",
    "plt.title(r'MSE for different values of $\\theta_0$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Based on the above plot, without calculation, what is the value of $\\theta_0$ that minimizes the MSE? Round your answer to the nearest integer and assign it to `theta0_hat_observed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta0_hat_observed = ...\n",
    "theta0_hat_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Numerical Solution\n",
    "\n",
    "Similar to the previous questions, we can use numerical optimization to estimate the optimal parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Using the `minimize` function, assign `theta0_hat_scipy` to the value of $\\theta_0$ that minimizes the MSE for our `tips` dataset. In other words, you want to find the exact minimum of the plot that you saw in the previous part.\n",
    "\n",
    "**Notes:** \n",
    "* You should use the function you defined earlier: `mse_tips_constant`.\n",
    "* For autograding purposes, assign `theta0_hat_scipy` to the value of $\\theta_0$ that minimizes the MSE according to the `minimize` function, called with initial `x0 = 0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call minimize with initial x0 = 0.0.\n",
    "theta0_hat_scipy = ...\n",
    "theta0_hat_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "## Analytical Solution\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3d\n",
    "\n",
    "As you saw in the lecture, we show that the value of $\\theta_0$ that minimizes the MSE for the constant model is the average (mean) of the data. Assign `theta0_hat_analytical` to the mean of the observed `y_tips` values, and compare this to the values you observed in questions 3b and 3c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta0_hat_analytical = ...\n",
    "theta0_hat_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to compare your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('theta0_hat_observed: ', theta0_hat_observed)\n",
    "print('theta0_hat_analytical: ', theta0_hat_analytical)\n",
    "print('theta0_hat_numerical: ', theta0_hat_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Reflecting on Question 3, we used a 3-step approach to find the \"best\" summary statistic $\\theta_0$:\n",
    "\n",
    "A. Define the constant model $\\hat{y}=\\theta_0$.\n",
    "\n",
    "B. Define \"best\": Define loss per datapoint ($L_2$ loss) and consequently define the model's loss $R(\\theta_0)$ over a given data array as the Mean Squared Error (MSE).\n",
    "\n",
    "C. Find the $\\theta_0 = \\hat{\\theta_0}$ that minimizes the $R(\\theta_0)$ in several ways:\n",
    "* **Visually**: Create a plot of $R(\\theta_0)$ vs. $\\theta_0$ and eyeball the minimizing $\\hat{\\theta_0}$.\n",
    "* **Numerically**: Create a function that returns $R(\\theta_0)$ for the given data for a given $\\theta_0$, and use the scipy `minimize` function to find the minimizing $\\hat{\\theta_0}$.\n",
    "* **Analytically**: Use calculus to find $\\hat{\\theta_0}$ that minimizes MSE $R(\\theta_0)$. Then compute $\\hat{\\theta_0}$ as the mean of the given data array, since this minimizes the defined $R(\\theta_0)$.\n",
    "    \n",
    "At this point, you've hopefully convinced yourself that the mean of the data is the summary statistic that minimizes the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our prediction for every meal's tip**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define the prediction function, no futher action needed.\n",
    "def predict_tip_constant():\n",
    "    return theta0_hat_analytical\n",
    "\n",
    "# Do not edit below this line.\n",
    "bill = 20\n",
    "print(f\"\"\"No matter what meal you have, Question 3 modeling process\n",
    "    predicts that you will pay a tip of ${predict_tip_constant():.2f}.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Compare the Constant and Linear Models with L2 Loss\n",
    "Both the linear model (Question 1) and constant model (Question 3) were optimized using the same $L_2$ loss function but predicted different values for different tips.\n",
    "\n",
    "Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate linear vs constant prediction\n",
    "\n",
    "sns.scatterplot(x = x_total_bills, y = y_tips, label='observed');\n",
    "\n",
    "# The below plots expect that you've run all other cells before this one\n",
    "plt.plot(x_total_bills, predict_tip_linear(x_total_bills), label='linear', color='g');\n",
    "plt.axhline(y=predict_tip_constant(), label='constant', color='m', ls='--');\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"total bill\")\n",
    "plt.ylabel(\"tip\")\n",
    "plt.title(\"Tips: Linear vs Constant Models\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while we plot the tip against the total bill, the constant model doesn't use the total bill in its prediction and therefore shows up as a horizontal line.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "### (Ungraded) Question 3e: Thought Question\n",
    "\n",
    "For predicting tips on this data, would you rather use the constant model or the linear model, assuming an $L_2$ loss function for both? Note that your answer will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Thor congratulates you for finishing Lab 06!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='thor.jpg' width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://forms.gle/owfPCGgnrju1xQEA9). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Submit this file to the Lab 06 assignment on Gradescope. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(mse_tips_linear(0, 0), 10.896283606557375)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mse_tips_linear(0.9, 0.1), 1.052336405737705)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mse_tips_linear(5, 0), 5.913496721311476)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(x_bar, 19.78594262295082)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(y_bar, 2.99827868852459)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(std_x, 8.884150577771132)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(std_y, 1.3807999538298954)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(r, float)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(r, 0.6757341092113641)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(theta1_hat, 0.10502451738435334)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(theta0_hat, 0.9202696135546735)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(predict_tip_linear(20), 3.0207599279976063)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1e": {
     "name": "q1e",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(mse_tips_linear_list([0, 1]), 346.0816225409836)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mse_tips_linear_list([5, 0]), 5.913496721311476)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mse_tips_linear_list([0, 0]), 10.896283606557375)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1f": {
     "name": "q1f",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(min_scipy_linear['x'][0], 0.9202703450693733)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(mae_tips_linear(5.3, 2), 41.87360655737705)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mae_tips_linear(1, 10), 195.8611475409836)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mae_tips_linear(20, 50), 1006.2988524590164)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(min_scipy_linear_mae['x'][0], 0.5180755855011097)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(mse_tips_constant(5.3), 7.20452950819672)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mse_tips_constant(1), 5.899726229508197)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mse_tips_constant(20), 290.96513606557374)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> theta0_hat_observed == 3\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(theta0_hat_scipy, 2.9982787346405537)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 2.99 <= theta0_hat_analytical <= 3\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "301px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
