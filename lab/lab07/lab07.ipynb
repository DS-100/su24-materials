{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab07.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: OLS\n",
    "## Due Sunday, July 14, 11:59 PM PT\n",
    "\n",
    "In this lab, you will review the details of linear regression. In particular:\n",
    "\n",
    "* How to formulate Matrices and solutions to Ordinary Least Squares (OLS).\n",
    "* `sns.lmplot` as a quick visual for Simple Linear Regression (SLR).\n",
    "* `scikit-learn`, or `sklearn` for short, a real-world data science tool that is more robust and flexible than analytical or `scipy.optimize` solutions. \n",
    "\n",
    "You will also practice interpreting residual plots (vs. fitted values) and the Multiple $R^2$ metric used in Multiple Linear Regression.\n",
    "\n",
    "To receive credit for a lab, answer all questions correctly and submit before the deadline.\n",
    "\n",
    "**The on-time deadline is Sunday, July 14, 11:59 PM PT**. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted. While course staff is happy to help you if you encounter difficulties with submission, we may not be able to respond to late-night requests for assistance (TAs need to sleep, after all!). **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to contact staff for submission support.\n",
    "\n",
    "**This lab contains five written questions**, which will be graded based on completion and coherence. After submitting this assignment to the Lab 07 Coding assignment on Gradescope, Gradescope will automatically submit the PDF from this file to the Lab 07 Written assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk to others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "**Note:** This video is recorded in Spring 2022. There may be slight inconsistencies between the version you are viewing and the version used in the recording, but content is identical. Particularly, Question 4 is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"IkkhAr3e19Q\", list = 'PLQCcNQgUcDfpuwnASdUyvQky51ZcYMWSy', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Debugging Guide\n",
    "\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Gradescope, and common `pandas` and RegEx errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "For the first part of this lab, you will predict fuel efficiency (`mpg`) of several models of automobiles using a **single feature**: engine power (`horsepower`). For the second part, you will perform feature engineering on **multiple features** to better predict fuel efficiency.\n",
    "\n",
    "First, let's load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we load the fuel dataset, and drop any rows that have missing data.\n",
    "vehicle_data = sns.load_dataset('mpg').dropna()\n",
    "vehicle_data = vehicle_data.sort_values('horsepower', ascending=True)\n",
    "vehicle_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 392 datapoints and 8 potential features (plus our observed $y$ values, `mpg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit a line to the plot below, which shows `mpg` vs. `horsepower` for several models of automobiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the data. \n",
    "sns.scatterplot(data=vehicle_data, x='horsepower', y='mpg');\n",
    "plt.title(\"mpg vs horsepower\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Ordinary Least Squares\n",
    "Recall that the equation for Simple Linear Regression (SLR) has two $\\theta$ coefficients: $\\theta_0$ and $\\theta_1$, and is written as follows:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "\n",
    "If we have many pairs of $(x_i, y_i)$ values, $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$, we would need to apply the SLR equation to each pair $n$ times. This is quite repetitive, so let's re-formulate our SLR equation using linear algebra. We'll:\n",
    "\n",
    "* Rewrite our $\\theta$ coefficients as a vector $\\theta = [\\theta_0, \\theta_1]$.\n",
    "* Stack our $x_i$ values into a vector $\\vec{x}$ with $n$ values.\n",
    "* Stack our $y_i$ values into a vector $\\mathbb{Y}$ of all $n$ observations in our sample.\n",
    "\n",
    "Then our prediction vector $\\hat{\\mathbb{Y}}$ can then be written as:\n",
    "$$\\Large \\hat{\\mathbb{Y}} = {\\theta_0} \\vec{1}_n + {\\theta_1} \\vec{x} = \\begin{bmatrix} | & | \\\\ \\vec{1}_n & \\vec{x} \\\\ | & | \\end{bmatrix} \\begin{bmatrix} {\\theta_0} \\\\ {\\theta_1} \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} {\\theta_0} \\\\ {\\theta_1} \\end{bmatrix} = \\Bbb{X} \\begin{bmatrix} {\\theta_0} \\\\ {\\theta_1} \\end{bmatrix} = \\mathbb{X} \\theta$$\n",
    "\n",
    "where $\\mathbb{X} \\in \\mathbb{R}^{n\\times2}$ is the **design matrix** with a **bias** column of all ones to account for the intercept, $\\theta_0$, and one **feature** for all $n$ datapoints in our sample. \n",
    "\n",
    "Our equation now matches the Ordinary Least Squares (OLS) equation! \n",
    "$$\\Large \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$\n",
    "\n",
    "\n",
    "### Expanding OLS to Multiple Linear Regression\n",
    "The OLS equation can be expanded to cases when we have more than one feature, like in the case of our Multiple Linear Regression (MLR) model, where we can have $p$ features: \n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_p x_p$$\n",
    "\n",
    "Our OLS equation can be generalized as  \n",
    "$$\\Large \\hat{\\mathbb{Y}} = \\begin{bmatrix} \n",
    "    1  & x_{1,1}  & x_{1,2}  & \\cdots & x_{1,p}\\\\\n",
    "    1  & x_{2,1}  & x_{2,2}    & \\cdots & x_{2,p}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    1  & x_{n,1}  & x_{n,2}    & \\cdots & x_{n,p}\n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix} {\\theta_0} \\\\ {\\theta_1} \\\\ \\vdots \\\\ \\theta_{p} \\end{bmatrix} = \\mathbb{X} \\theta\n",
    "$$\n",
    "with a\n",
    "* **prediction vector** $\\mathbb{Y} \\in \\mathbb{R}^{n}$,\n",
    "* **design matrix** $\\mathbb{X} \\in \\mathbb{R}^{n\\times(p + 1)}$ representing the $p$ features for all $n$ datapoints in our sample,\n",
    "* and a **parameter vector** $\\theta \\in \\mathbb{R}^{p + 1}$.\n",
    "\n",
    "Simple linear regression is a special case of OLS when $p=1$.\n",
    "\n",
    "### Today's Lab\n",
    "\n",
    "In today's lab, we'll explore the OLS equations with different examples.\n",
    "* In Question 1, we'll write code to evaluate linear algebra and apply those functions to predict `mpg` from one feature, `horsepower`. Since we only have one feature ($p=1$), this is a case of simple linear regression.\n",
    "* In Question 2, we'll explore how transforming data affects our prediction by using horsepower squared (`hp^2`) as our feature instead of `horsepower`. \n",
    "* In Question 3, we'll combine the features from questions 1 and 2 and use *multiple linear regression* on 2 features: `horsepower` and `hp^2`.\n",
    "* Finally for question 4, we'll explore how redundant features affect our calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1a: Construct $\\mathbb{X}$ with an intercept term\n",
    "The OLS equation is displayed for your reference: \n",
    "$$\\Large \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$\n",
    "\n",
    "Because we have an intercept term $\\theta_0$ in our parameter vector $\\theta$, our design matrix $\\mathbb{X}$ needs a column with all-ones such that the resulting matrix expression, $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$, represents $n$ linear equations, where equation $i$ is \n",
    "$$\\hat{y_i} = \\theta_0 \\cdot 1 + \\theta_1 \\cdot x_{i, 1} + \\dots + \\theta_p \\cdot x_{i, p}$$ \n",
    "where $x_{i, j}$ is the $j^{th}$ feature of the $i^{th}$ datapoint. The constant all-ones column of $\\mathbb{X}$ is sometimes called the bias feature; $\\theta_0$ is frequently called the **bias or intercept term**. \n",
    "\n",
    "> _Note:_ <span style=\"color:gray\">\n",
    "At other points in the course, and by convention, we may represent the model using an equivalent expression written without the index $i$, namely:\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_{1} + \\dots + \\theta_p x_{p}$$\n",
    "> When written out like this, the symbols $x_{i, j}$ and $x_{j}$ are functionally identical - both refer to the $j^{th}$ feature of the $i^{th}$ datapoint. The $i$ is implicit in the case of the latter, but we are still talking in terms of numerical values, not vectors.\n",
    "</span>\n",
    "\n",
    "\n",
    "In order to construct the design matrix $\\mathbb{X} \\in \\mathbb{R}^{n\\times(p + 1)}$ from a given `DataFrame`, `X`, with a dimension of $n$ rows by $p$ columns, we need to augment our `DataFrame` with a column of ones.\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "Below, implement `add_intercept`, which creates a design matrix such that the first (left-most) column is all ones. The function has two lines: you are responsible for constructing the all-ones column `bias_feature` using the `np.ones` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.ones.html?highlight=ones)). This is then piped into a call to `np.concatenate` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html)), which we've implemented for you.\n",
    "\n",
    "**Note:** `bias_feature` should be a matrix of dimension `(n,1)`, not a vector of dimension `(n,)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    \"\"\"\n",
    "    Return X with a bias feature.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D DataFrame of p numeric features\n",
    "    (may also be a 2D NumPy array) of shape n x p\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 2D matrix of shape n x (p + 1), where the leftmost\n",
    "    column is a column vector of 1's.\n",
    "    \"\"\"\n",
    "    bias_feature = ...\n",
    "    return np.concatenate([bias_feature, X], axis=1)\n",
    "\n",
    "# Note the [[ ]] brackets below: the argument needs to be\n",
    "# a matrix (DataFrame), as opposed to a single array (Series).\n",
    "X = add_intercept(vehicle_data[['horsepower']])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1b: Define the OLS Model\n",
    "\n",
    "The predictions for all $n$ points in our data are:\n",
    "$$ \\Large \\hat{\\mathbb{Y}} = \\mathbb{X}\\theta $$\n",
    "where $\\theta = [\\theta_0, \\theta_1, \\dots, \\theta_p]$.\n",
    "\n",
    "Below, implement the `linear_model` function to evaluate this product.\n",
    "\n",
    "**Hint**: You can use `np.dot` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)), `pd.DataFrame.dot` ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dot.html)), or the `@` operator to multiply matrices/vectors. However, while the `@` operator can be used to multiply `NumPy` arrays, it generally will not work between two `pandas` objects, so keep that in mind when computing matrix-vector products!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_model(thetas, X):\n",
    "    \"\"\"\n",
    "    Return the linear combination of thetas and features as defined in the OLS equation.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    thetas: a 1D vector representing the parameters of our model ([theta0, theta1, ...]).\n",
    "    X: a 2D DataFrame of numeric features (may also be a 2D NumPy array).\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 1D vector representing the linear combination of thetas and features as defined in the OLS equation.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c: Least Squares Estimate, Analytically\n",
    "\n",
    "Recall from lecture that Ordinary Least Squares is when we fit a linear model using Mean Squared Error (MSE), which is equivalent to the following optimization problem:\n",
    "\n",
    "$$\\Large \\min_{\\theta} ||\\Bbb{X}\\theta - \\Bbb{Y}||^2$$\n",
    "\n",
    "We showed in lecture that when $X^TX$ is invertible, the optimal estimate, $\\hat{\\theta}$, is given by the equation:\n",
    "\n",
    "$$ \\Large \\hat{\\theta} = (\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T\\Bbb{Y}$$\n",
    "\n",
    "Below, implement the analytic solution to $\\hat{\\theta}$ using `np.linalg.inv` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)) to compute the inverse of $\\Bbb{X}^T\\Bbb{X}$.\n",
    "\n",
    "**Hint 1**: To compute the transpose of a matrix, you can use `X.T` or `X.transpose()` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html#numpy.ndarray.T)).\n",
    "\n",
    "**Note:** You can also consider using `np.linalg.solve` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)) instead of `np.linalg.inv` because it is more robust (more on StackOverflow [here](https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_analytical_sol(X, y):\n",
    "    \"\"\"\n",
    "    Computes the analytical solution to our\n",
    "    least squares problem\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D DataFrame (or NumPy array) of numeric features.\n",
    "    y: a 1D vector of outputs.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimate for theta (a 1D vector) computed using the\n",
    "    equation mentioned above.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "Y = vehicle_data['mpg']\n",
    "analytical_thetas = get_analytical_sol(X, Y)\n",
    "analytical_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now, let's analyze our model's performance. Your task will be to interpret the model's performance using the two visualizations and one performance metric we've implemented below.\n",
    "\n",
    "First, we run `sns.lmplot`, which will both provide a scatterplot of `mpg` vs `horsepower` and display the least-squares line of best fit. (If you'd like to verify the OLS fit you found above is the same line found through `Seaborn`, change `include_OLS` to `True`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_OLS = False # Change this flag to visualize OLS fit\n",
    "\n",
    "sns.lmplot(data=vehicle_data, x='horsepower', y='mpg');\n",
    "predicted_mpg_hp_only = linear_model(analytical_thetas, X)\n",
    "if include_OLS:\n",
    "    # if flag is on, add OLS fit as a dotted red line\n",
    "    plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_only, 'r--')\n",
    "plt.title(\"mpg vs horsepower\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **plot the residuals.** While in Simple Linear Regression we have the option to plot residuals vs. the single input feature, in Multiple Linear Regression we often plot residuals vs. fitted values $\\hat{\\mathbb{Y}}$. In this lab, we opt for the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predicted_mpg_hp_only, Y - predicted_mpg_hp_only)\n",
    "plt.axhline(0, c='black', linewidth=1)\n",
    "plt.xlabel(r'Fitted Values $\\hat{\\mathbb{Y}}$')\n",
    "plt.ylabel(r'Residuals $\\mathbb{Y} - \\hat{\\mathbb{Y}}$');\n",
    "plt.title(\"Residual plot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we compute the **correlation r** and **Multiple $R^2$** metric. As described in Lecture 12,\n",
    "\n",
    "$$R^2 = \\frac{\\text{variance of fitted values}}{\\text{variance of true } y} = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}$$\n",
    "\n",
    "$R^2$  can be used\n",
    "in the multiple regression setting, whereas $r$ (the correlation coefficient) is restricted to SLR since it depends on a single input feature.  In SLR, $r^{2}$ and Multiple $R^{2}$ are\n",
    "equivalent; the proof is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_hp_only = np.corrcoef(X[:, 1], Y)[0, 1]\n",
    "r2_hp_only = r_hp_only ** 2\n",
    "R2_hp_only = np.var(predicted_mpg_hp_only) / np.var(Y)\n",
    "\n",
    "print('Correlation, r, using only horsepower: ', r_hp_only)\n",
    "print('Correlation squared, r^2, using only horsepower: ', r2_hp_only)\n",
    "print('Multiple R^2 using only horsepower: ', r2_hp_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1d\n",
    "\n",
    "In the cell below, comment on the above visualization and performance metrics, and whether `horsepower` and `mpg` have a good linear fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Transform a Single Feature\n",
    "\n",
    "The Tukey-Mosteller Bulge Diagram (shown below) tells us to transform our $\\mathbb{X}$ or $\\mathbb{Y}$ to find a linear fit.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"tukey_mosteller.png\" width=\"300vw\" /></div>\n",
    "\n",
    "Let's consider the following linear model:\n",
    "\n",
    "$$\\text{predicted mpg} = \\theta_0 + \\theta_1 \\sqrt{\\text{horsepower}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 2a\n",
    "\n",
    "In the cell below, explain why we use the term \"linear\" to describe the model above, even though it incorporates a square root of horsepower  as a feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Introduction to `sklearn`\n",
    "\n",
    "Another way to fit a linear regression model is to use `scikit-learn`, an industry-standard package for machine learning applications. Because it is application-specific, `sklearn` is often faster and more robust than the analytical or `scipy`-based computation methods we've used thus far. Note that `scikit-learn` and `sklearn` refers to the same package, but it can only be imported under the name `sklearn`. We will use these two names interchangeably in this class.\n",
    "\n",
    "To use `sklearn`:\n",
    "\n",
    "1. Create an `sklearn` object.\n",
    "1. `fit` the object to data.\n",
    "1. Analyze fit or call `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Create object.** \n",
    "\n",
    "We first create a `LinearRegression` object. Here's the `sklearn` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Note that by default, the object will include an intercept term when fitting.\n",
    "\n",
    "Here, `model` is like a \"blank slate\" for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run this cell to initialize a sklearn LinearRegression object.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# the `fit_intercept` argument controls whether or not the model should have an intercept (or bias) term\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. `fit` the object to data.** \n",
    "\n",
    "Now, we need to tell `model` to \"fit\" itself to the data. Essentially, this is doing exactly what you did in the previous part of this lab (creating a risk function and finding the parameters that minimize that risk).\n",
    "\n",
    "**Note**: `X` needs to be a matrix (or `DataFrame`), as opposed to a single array (or `Series`) when running `model.fit`. This is because `sklearn.linear_model` is robust enough to be used for multiple regression, which we will look at later in this lab. This is why we use the double square brackets around `sqrt(hp)` when passing in the argument for `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Run this cell to add sqrt(hp) column for each car in the dataset.\n",
    "vehicle_data['sqrt(hp)'] = np.sqrt(vehicle_data['horsepower'])\n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Run this cell to fit the model to the data.\n",
    "model.fit(X = vehicle_data[['sqrt(hp)']], y = vehicle_data['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Analyze fit.** \n",
    "\n",
    "Now that the model exists, we can look at the $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$ values it found, which are given in the attributes `intercept` and `coef`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `sklearn` linear regression model to make predictions, you can use the `model.predict` method.\n",
    "\n",
    "Below, we find the estimated `mpg` for a single datapoint with a `sqrt(hp)` of 6.78 (i.e., horsepower 46). Unlike the linear algebra approach, we do not need to manually add an intercept term because our `model` (which was created with `fit_intercept=True`) will automatically add one.\n",
    "\n",
    "**Note:** You may receive a user warning about missing feature names. This is due to the fact that we fitted on the feature DataFrame `vehicle_data[['sqrt(hp)']]` with feature names `\"sqrt(hp)\"` but only pass in a simple 2D arrays for prediction. To avoid this, we can convert our 2D array into a DataFrame with the matching feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be a 2D array since the X in step 2 was 2-dimensional.\n",
    "single_datapoint = [[6.78]]\n",
    "# Uncomment the following to see the result of predicting on a DataFrame instead of 2D array.\n",
    "#single_datapoint = pd.DataFrame([[6.78]], columns = ['sqrt(hp)']) # \n",
    "model.predict(single_datapoint) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "Using the model defined above, which takes in `sqrt(hp)` as an input explanatory variable, predict the `mpg` for the full `vehicle_data` dataset. Assign the predictions to `predicted_mpg_hp_sqrt`. Running the cell will then compute the multiple $R^2$ value and create a linear regression plot for this new square root feature, overlaid on the original least squares estimate (used in Question 1c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_mpg_hp_sqrt = ...\n",
    "\n",
    "# Do not modify below this line.\n",
    "r2_hp_sqrt = np.var(predicted_mpg_hp_sqrt) / np.var(vehicle_data['mpg'])\n",
    "print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt)\n",
    "\n",
    "sns.lmplot(x = 'horsepower', y = 'mpg', data = vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_sqrt,\n",
    "         color = 'r', linestyle='--', label='sqrt(hp) fit');\n",
    "plt.title(\"mpg vs. horsepower\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "The visualization shows a slight improvement, but the points on the scatter plot are still more \"curved\" than our prediction line. Let's try a quadratic feature instead! \n",
    "\n",
    "Next, we use the power of OLS to **add an additional feature.** Questions 1 and 2 utilized simple linear regression, a special case of OLS where we have 1 feature ($p=1$). For the following questions, we'll utilize multiple linear regression, which are cases of OLS when we have more than 1 features ($p > 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Add an Additional Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of this lab, we move from SLR to multiple linear regression.\n",
    "\n",
    "Until now, we have established relationships between one independent explanatory variable and one response variable. However, with real-world problems, you will often want to use **multiple features** to model and predict a response variable. Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to the observed data.\n",
    "\n",
    "We can consider including functions of existing features as **new features** to help improve the predictive power of our model. (This is something we will discuss in further detail in the Feature Engineering lecture.)\n",
    "\n",
    "The cell below adds a column that contains the square of the horsepower for each car in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to add a column of horsepower squared, no further action needed.\n",
    "vehicle_data['hp^2'] = vehicle_data['horsepower'] ** 2\n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "## Question 3\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "Using `sklearn`'s `LinearRegression`, create and fit a model that tries to predict `mpg` from `horsepower` AND `hp^2` using the DataFrame `vehicle_data`. Name your model `model_multi`.\n",
    "\n",
    "**Hint**: It should follow a similar format as Question 2.\n",
    "\n",
    "**Note**: You must create a new model again using `LinearRegression()`, otherwise the old model from Question 2 will be overwritten. If you do overwrite it, don't fret! Just restart your kernel and run your cells in order. If you are unsure why this overwritting happens, please review [object-oriented programming](https://cs61a.org/study-guide/objects/) from CS61A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_multi = LinearRegression() # By default, fit_intercept=True\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can see the coefficients and intercept. Note that there are now two elements in `model_multi.coef_`, since there are two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Using the above values, write out the function that the model is using to predict `mpg` from `horsepower` and `hp^2`.\n",
    "\n",
    "**Note**: If you are using LaTeX, please keep the `^2` outside of any `\\text` curly brackets! For example, use `\\text{example}^2` instead of `\\text{example^2}`. This will help us avoid PDF compilation errors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "The plot below shows the prediction of our model. It's much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to show the prediction of our model.\n",
    "predicted_mpg_multi = model_multi.predict(vehicle_data[['horsepower', 'hp^2']])\n",
    "r2_multi = np.var(predicted_mpg_multi) / np.var(vehicle_data['mpg'])\n",
    "print('Multiple R^2 using both horsepower and horsepower squared: ', r2_multi)\n",
    "\n",
    "sns.scatterplot(x = 'horsepower', y = 'mpg', data = vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_only, label='hp only');\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_sqrt, color = 'r', linestyle='--', label='sqrt(hp) fit');\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_multi, color = 'gold', linewidth=2, label='hp and hp^2');\n",
    "plt.title(\"mpg vs horsepower\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By incorporating a squared feature, we are able to capture the curvature of the dataset. Our model is now a parabola centered on our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "In the cell below, we assign the mean of the `mpg` column of the `vehicle_data` DataFrame to `mean_mpg`. Given this information, what is the mean of the `mean_predicted_mpg_hp_only`, `predicted_mpg_hp_sqrt`, and `predicted_mpg_multi` arrays?\n",
    "\n",
    "**Hint**: Your answer should be a function of `mean_mpg` provided, you should not have to call `np.mean` in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_mpg = np.mean(vehicle_data['mpg'])\n",
    "mean_predicted_mpg_hp_only = ...\n",
    "mean_predicted_mpg_hp_sqrt = ...\n",
    "mean_predicted_mpg_multi = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this model with previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares q1, q2, q3, and overfit models (ignores redundant model)\n",
    "print('Multiple R^2 using only horsepower: ', r2_hp_only)\n",
    "print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt)\n",
    "print('Multiple R^2 using both hp and hp^2: ', r2_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the R^2 value of the last model is the highest. In fact, it can be proven that multiple R^2 will not decrease as we add more variables. You may be wondering, what will happen if we add more variables? We will discuss the limitations of adding too many variables in an upcoming lecture. Below, we consider an extreme case that we include a variable twice in the model.\n",
    "\n",
    "You might also be wondering why we chose to use `hp^2` as our additional feature, even though that transformation in the Tukey-Mosteller Bulge Diagram doesn't correspond to the bulge in our data. The Bulge diagram is a good starting point for transforming our data, but you may need to play around with different transformations to see which of them is able to capture the true relationship in our data and create a model with the best fit. This trial and error process is a very useful technique used all throughout data science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Faulty Feature Engineering: Redundant Features\n",
    "\n",
    "Suppose we used the following linear model:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{mpg} &= \\theta_0 + \\theta_1 \\cdot \\text{horsepower} + \\theta_2 \\cdot \\text{horsepower}^2 + \\theta_3 \\cdot \\text{horsepower} + \\theta_4 \\cdot \\text{horsepower}\n",
    "\\end{align}\n",
    "\n",
    "Notice that `horsepower` appears three times in our model!! We will explore how this redundant feature affects our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "### Question 4a: Linear Algebra\n",
    "\n",
    "Construct a matrix `X_redundant` that uses the `vehicle_data` DataFrame to encode the \"four\" features above, as well as a bias feature.\n",
    "\n",
    "**Hint**: Use the `add_intercept` term you implemented in Question 1a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_redundant = ...\n",
    "X_redundant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, run the cell below to find the analytical OLS Estimate using the `get_analytical_sol` function you wrote in Question 1c.\n",
    "\n",
    "**Note:** Depending on the machine that you run your code on, you should either **see a singular matrix error** or **end up with thetas that are nonsensical** (magnitudes greater than $10^{15}$). In other words, if the cell below errors, that is by design, it is supposed to error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed. \n",
    "# The try-except block suppresses errors during submission\n",
    "import traceback\n",
    "try:\n",
    "    analytical_thetas = get_analytical_sol(X_redundant, vehicle_data['mpg'])\n",
    "    print(analytical_thetas)\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4b\n",
    "\n",
    "In the cell below, explain why we got the error above when trying to calculate the analytical solution to predict `mpg`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "**Note 1**: While we encountered errors when using the linear algebra approach, a model fitted with `sklearn` will not encounter matrix singularity errors since it uses numerical methods to find optimums (to be covered in the Gradient Descent lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed. \n",
    "# sklearn finds optimal parameters despite redundant features\n",
    "model_redundant = LinearRegression(fit_intercept=False) # X_redundant already has an intercept column\n",
    "model_redundant.fit(X = X_redundant, y = vehicle_data['mpg'])\n",
    "model_redundant.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2**: In some situations, `np.linalg.inv` is actually able to approximately \"invert\" non-invertible matrices! In fact, if we had only added one redundant `horsepower` column, `get_analytical_sol` would have succeeded in estimating the OLS solution despite $X^TX$ being **non-invertible**. If you're interested in reading more, see [this Stack Overflow post](https://stackoverflow.com/questions/41841509/numpy-inverts-a-non-invertible-matrix).\n",
    "\n",
    "Run the cell below to see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed. \n",
    "X_less_redundant = add_intercept(vehicle_data[['horsepower', 'hp^2', 'horsepower']])\n",
    "try:\n",
    "    analytical_thetas = get_analytical_sol(X_less_redundant, vehicle_data['mpg'])\n",
    "    print(analytical_thetas)\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we actually check $(X^TX)^{-1}(X^TX)$, we can see that the resulting value is not the identity matrix. Remember that for any invertible matrix $A$, $A^{-1}A$ should be equivalent to the identity matrix. This corroborates that the matrix `np.linalg.inv` is outputting here is not the inverse, it's just an estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed. \n",
    "np.linalg.inv(X_less_redundant.T @ X_less_redundant) @ (X_less_redundant.T @ X_less_redundant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowy congratulates you for finishing Lab 07!\n",
    "\n",
    "<img src='snowy.jpg' width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://forms.gle/owfPCGgnrju1xQEA9). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Submit this file to the Lab 07 Coding assignment on Gradescope. Gradescope will automatically submit the PDF from this file to the Lab 07 Written assignment. There is no need to manually submit Lab 07 Written answers; however, please check that the PDF was generated and submitted correctly. If you run into any issues when running this cell, feel free to check this section in the Data 100 Debugging Guide. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X.shape == (392, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (add_intercept(np.array([[1, 2, 3], [4, 5, 6]]).T)[:, 0] == np.ones((3,))).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> add_intercept(np.array([[1, 2, 3], [4, 5, 6]]).T).shape == (3, 3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (add_intercept(np.array([[1, 2, 3], [4, 5, 6]]).T)[:, 2] == np.array([4, 5, 6])).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> linear_model(np.arange(1, 5), np.arange(1, 5)) == 30\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (linear_model(2 * np.eye(100), np.ones(100)) == 2 * np.ones(100)).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_theta = np.array([[1, 2], [3, 4], [5, 6]])\n>>> test_x = np.array([[1, 3, 5], [2, 4, 6]])\n>>> expected = np.array([[35, 44], [44, 56]])\n>>> actual = linear_model(test_theta, test_x)\n>>> np.array_equal(actual, expected)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_theta_2 = np.array([[3], [5]])\n>>> test_x_2 = np.array([[1, 4], [1, 6], [1, 8]])\n>>> expected_2 = np.array([[23], [33], [43]])\n>>> actual_2 = linear_model(test_theta_2, test_x_2)\n>>> np.array_equal(expected_2, actual_2)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> analytical_thetas.shape in ((2,), (2, 1))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(analytical_thetas[0], 39.93586102)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(model_multi.intercept_, 56.90009970211295)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(model_multi.coef_[0], -0.46618963)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(model_multi.coef_[1], 0.00123054)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(model.intercept_, 58.70517203721748) and np.isclose(model.coef_[0], -3.50352375)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(mean_predicted_mpg_hp_only, np.mean(predicted_mpg_hp_only), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_predicted_mpg_hp_sqrt, np.mean(predicted_mpg_hp_sqrt), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_predicted_mpg_multi, np.mean(predicted_mpg_multi), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_redundant.shape == (392, 5)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
