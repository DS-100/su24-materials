{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab08.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 08: Gradient Descent and `sklearn`\n",
    "## Due Wednesday, July 17, 11:59 PM PT\n",
    "\n",
    "In this lab, we will work through the process of:\n",
    "1. Defining loss functions,\n",
    "1. Performing feature engineering,\n",
    "1. Minimizing loss functions using numerical methods and analytical methods,\n",
    "1. Understanding what happens if we use the analytical solution for OLS on a matrix with redundant features,\n",
    "1. Computing a gradient for a nonlinear model, and\n",
    "1. Using gradient descent to optimize the non-linear model.\n",
    "\n",
    "This lab will continue using the toy `tips` calculation dataset used in a prior lab.\n",
    "\n",
    "To receive credit for a lab, answer all questions correctly and submit before the deadline.\n",
    "\n",
    "**The on-time deadline is Wednesday, July 17, 11:59 PM PT**. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted. While course staff is happy to help you if you encounter difficulties with submission, we may not be able to respond to late-night requests for assistance (TAs need to sleep, after all!). **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to contact staff for submission support.\n",
    "\n",
    "**This lab contains two written questions**, which will be graded based on completion and coherence. After submitting this assignment to the Lab 08 Coding assignment on Gradescope, Gradescope will automatically submit the PDF from this file to the Lab 08 Written assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "**Note:** Some parts of the video are recorded in Spring 2022. There may be slight inconsistencies between the version you are viewing and the version used in the recording, but the content is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"LohVOmiulHQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk to others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Debugging Guide\n",
    "\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Gradescope, and common `pandas`, RegEx, and visualization errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data-text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "To begin, let's load the tips dataset from the `seaborn` library.  This dataset contains records of tips, total bill, and information about the person who paid the bill. As earlier, we'll be trying to predict tips from the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the tips dataset; no further action is needed.\n",
    "data = sns.load_dataset(\"tips\")\n",
    "\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Functions\n",
    "\n",
    "So far, we've only considered models of the form $\\hat{y} = f_{\\theta}(x) = \\theta_0 + \\sum_{j=1}^p x_j\\theta_j$, where $\\hat{y}$ is a quantitative continuous variable. \n",
    "\n",
    "We call this a linear model because it is a linear combination of the features $x_1, \\dots, x_p$. However, our features don't need to be numbers: we could have categorical values such as names. Additionally, the true relationship doesn't have to be linear, as we could have a relationship that is quadratic, such as the relationship between the height of a projectile and time.\n",
    "\n",
    "In these cases, we often apply **feature functions**, functions that take in some value and output another value. This might look like converting a string into a number, combining multiple numeric values, or creating a boolean value from some filter.\n",
    "\n",
    "If we use $\\phi$ to represent the feature (_\"phi\"_-ture) function or transformation applied to our data, then our model takes the following form: $$\\hat{y} = f_{\\theta}(x) = \\theta_0 + \\sum_{j=1}^p \\phi(x)_j\\theta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Feature Functions\n",
    "\n",
    "1. **One-hot encoding**\n",
    "    - Converts a single categorical feature into many binary features, each of which represents one of the possible values in the original column.\n",
    "    - Each of the binary feature columns produced contains a 1 for rows that had that column's label in the original column and 0 elsewhere.\n",
    "1. **Polynomial feature**\n",
    "    - Creates polynomial combinations of features.\n",
    "1. **Normalized/Standardized feature**\n",
    "    - Normalizes features so they have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Defining the Model and Engineering Features\n",
    "\n",
    "In Lab 5, we used both a Simple Linear Regression (SLR) model and a constant model on this dataset. Now, let's make a more complicated model that utilizes other features in our dataset. You can imagine that we might want to use the features with an equation that looks as shown below:\n",
    "\n",
    "$$ \\text{Tip} = \\theta_0 + \\theta_1 \\cdot \\text{total}\\_\\text{bill} + \\theta_2 \\cdot \\text{sex} + \\theta_3 \\cdot \\text{smoker} + \\theta_4 \\cdot \\text{day} + \\theta_5 \\cdot \\text{time} + \\theta_6 \\cdot \\text{size} $$\n",
    "\n",
    "Unfortunately, that's not possible because some of these features like \"day\" are not numbers, so it doesn't make sense to multiply by a numerical parameter. Let's start by converting some of these non-numerical values into numerical values.\n",
    "\n",
    "Before we do this, let's separate out the tips and the features into two separate variables, and add a bias term using `pd.insert` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.insert.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create our design matrix X; no further action is needed.\n",
    "tips = data['tip']\n",
    "X = data.drop(columns='tip')\n",
    "X.insert(0, 'bias', 1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Feature Engineering\n",
    "\n",
    "First, let's convert our features to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones. \n",
    "\n",
    "For example, we could convert the `day` feature to a numerical value from 1-7. However, one of the disadvantages of directly translating to a numeric value is that we unintentionally assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7, and Monday to the numeric value of 1. In our linear model, Sunday will have 7 times the influence of Monday, which can (and likely will) lower the performance of our model.\n",
    "\n",
    "Instead, let's use **one-hot encoding** to better represent these features!  As you learned in the lecture, one-hot encoding is a feature engineering method that represents non-numeric features using boolean vectors (numerical values 0 or 1).\n",
    "\n",
    "In the `tips` dataset, for example, we encode Sunday as the row vector `[0 0 0 1]` because our dataset only contains bills from Thursday through Sunday. This replaces the `day` feature with four boolean features indicating if the record occurred on Thursday, Friday, Saturday, or Sunday. One-hot encoding therefore assigns a more even weight across each category in non-numeric features.\n",
    "\n",
    "Complete the code below to one-hot encode our dataset. This `DataFrame` holds our \"featurized\" data, which is also often denoted by $\\phi$.\n",
    "\n",
    "**Hint 1:** You should use `sklearn`'s `OneHotEncoder` class ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)) when doing your one-hot encoding. Note that `OneHotEncoder` transforms data into a [SciPy sparse matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) to save space; we'll need to convert these back into regular arrays before doing any operations on them. Check out `.toarray()` ([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html)) for how to convert this to a `NumPy` array.\n",
    "\n",
    "**Hint 2:** Look through the lecture slides and/or course notes for examples of how to use `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded DataFrame of our input data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: A DataFrame that may include non-numerical features.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded DataFrame that only contains numeric features.\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return data\n",
    "\n",
    "    \n",
    "one_hot_X = one_hot_encode(X)\n",
    "one_hot_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "### Tutorial: `fit()`/`predict()`\n",
    "\n",
    "This tutorial serves to provide you with a blueprint for how to create a model. Now that all of our data is numeric, we can begin to define our model function. Notice that after one-hot encoding our data, we now have 13 features instead of 7 (including bias). Therefore, our linear model is now similar to the below (note the order of thetas below does not necessarily match the order in the `DataFrame`):\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Tip} &= \\theta_0 + \\theta_1 \\cdot \\text{total}\\_\\text{bill} + \\theta_2 \\cdot \\text{size}  \\\\\n",
    "& + \\theta_3 \\cdot \\text{sex}\\_\\text{Female} + \\theta_4 \\cdot \\text{sex}\\_\\text{Male} \\\\\n",
    "& + \\theta_5 \\cdot \\text{smoker}\\_\\text{No} + \\theta_{6} \\cdot \\text{smoker}\\_\\text{Yes} \\\\\n",
    "& + \\theta_7 \\cdot \\text{day}\\_\\text{Fri} + \\theta_8 \\cdot \\text{day}\\_\\text{Sat} + \\theta_9 \\cdot \\text{day}\\_\\text{Sun} + \\theta_{10} \\cdot \\text{day}\\_\\text{Thur} \\\\\n",
    "& + \\theta_{11} \\cdot \\text{time}\\_\\text{Dinner}+ \\theta_{12} \\cdot \\text{time}\\_\\text{Lunch} \n",
    "\\end{align}\n",
    "\n",
    "We can represent the linear combination above as a matrix-vector product. To practice using syntax similar to the `sklearn` pipeline, we introduce a staff-made example called `MyZeroLinearModel`.\n",
    "\n",
    "The `MyZeroLinearModel` has two methods, `predict` and `fit`.\n",
    "* `fit`: Compute parameters theta given data `X` and `Y` and the underlying model.\n",
    "* `predict`: Compute estimate $\\hat{y}$ given `X` and the underlying model.\n",
    "\n",
    "If you are unfamiliar with using `Python` objects, please review [object-oriented programming](https://cs61a.org/study-guide/objects/). \n",
    "\n",
    "**Note:** Practically speaking, this is a pretty bad model: it sets all of its parameters to 0 regardless of the data we fit it to! While this model doesn't really have any practical application, we're using it here to help you build intuition on how `sklearn` pipelines work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create the MyZeroLinearModel class; no further action is needed.\n",
    "class MyZeroLinearModel():\n",
    "    def __init__(self):\n",
    "        self._thetas = None\n",
    "    def fit(self, X, Y):\n",
    "        number_of_features = X.shape[1]\n",
    "        # For demonstration purposes in this tutorial, we set the values of all the parameters to 0. \n",
    "        self._thetas = np.zeros(shape=(number_of_features, 1))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "\n",
    "# Running the code below produces all-zero thetas\n",
    "model0 = MyZeroLinearModel()\n",
    "model0.fit(one_hot_X, tips)\n",
    "model0._thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Fitting a Linear Model Using Numerical Methods\n",
    "\n",
    "\n",
    "The best-fit model is determined by our loss function. Recall in Lab 5 and in Lecture 12 we defined multiple loss functions and found the optimal $\\hat{\\theta}$ using the `scipy.optimize.minimize` function. \n",
    "\n",
    "\n",
    "In this question, we'll wrap this function into a method `fit()` in our class `MyScipyLinearModel`.\n",
    "To allow for different loss functions, we create a `loss_function` parameter where the model can be fit accordingly. Example loss functions are given as `l1` and `l2`.\n",
    "\n",
    "**Note:** Just like `MyZeroLinearModel`, the class `MyScipyLinearModel` is a staff-made example to help you understand how `sklearn` works behind the scenes. In practice, when using pre-made `sklearn` models, defining a class like this is unnecessary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "### Question 2a: `scipy`\n",
    "\n",
    "Complete the code below using `scipy.optimize.minimize`. Find and store the optimal $\\hat{\\theta}$ in the instance attribute `self._thetas`.\n",
    "\n",
    "**Hint:**\n",
    "* The `starting_guess` should be some arbitrary array (such as an array of all zeroes) of the correct length. You may find `number_of_features` helpful.\n",
    "\n",
    "**Notes:**\n",
    "* Notice that `l1` and `l2` return term-wise loss and only accept observed value $y$ and predicted value $\\hat{y}$. We added a lambda function to help convert them into the right format for `scipy.optimize.minimize`.\n",
    "* Notice above that we extract the `'x'` entry in the dictionary returned by `minimize`. This entry corresponds to the optimal $\\hat{\\theta}$ estimated by the function, and it is the format that `minimize` uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def l1(y, y_hat):\n",
    "    return np.abs(y - y_hat)\n",
    "\n",
    "def l2(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "class MyScipyLinearModel():\n",
    "    def __init__(self):\n",
    "        self._thetas = None\n",
    "        \n",
    "    def fit(self, loss_function, X, Y):\n",
    "        \"\"\"\n",
    "        Estimated optimal _thetas for the given loss function, \n",
    "        feature matrix X, and observed values y. Store them in _thetas.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        loss_function: A function that takes in observed and predicted y, \n",
    "                       and return the loss calculated for each data point.\n",
    "        X: A 2D DataFrame (or NumPy array) of numeric features.\n",
    "        Y: A 1D NumPy array or Series of the dependent variable.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        number_of_features = X.shape[1]\n",
    "        starting_guess = ...\n",
    "        self._thetas = minimize(lambda theta:\n",
    "                                ...\n",
    "                                , x0 = starting_guess)['x']        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas\n",
    "        \n",
    "# Create a new model and fit the data using l2 loss, it should produce some non-zero thetas.\n",
    "model = MyScipyLinearModel()\n",
    "model.fit(l2, one_hot_X, tips)\n",
    "print(\"L2 loss thetas:\")\n",
    "print(model._thetas)\n",
    "\n",
    "# Create a new model and fit the data using l1 loss, it should produce some non-zero thetas.\n",
    "model_l1 = MyScipyLinearModel()\n",
    "model_l1.fit(l1, one_hot_X, tips)\n",
    "print(\"L1 loss thetas:\")\n",
    "print(model._thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE and MAE for your model above should be just slightly larger than 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to calculate the MSE and MAE of the above model; no further action is needed.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"L2 loss MSE scipy: \" + str(mean_squared_error(model.predict(one_hot_X), tips)))\n",
    "print(\"L1 loss MAE scipy: \" + str(mean_squared_error(model_l1.predict(one_hot_X), tips)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b: `sklearn`\n",
    "\n",
    "Another way to fit a linear regression model is to use `scikit-learn`/`sklearn` as we have seen in Lab 6.  As a reminder, here are the three steps to use `sklearn`:\n",
    "\n",
    "1. Create a `sklearn` object.\n",
    "1. `fit` the object to data.\n",
    "1. Analyze fit, or call `predict`.\n",
    "\n",
    "\n",
    "The `sklearn` `LinearRegression` object ([documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)) models the Ordinary Least Squares (OLS) problem, also using numerical methods to estimate $\\hat{\\theta}$. Fill in the code below such that `sklearn_model` **fits** OLS using `sklearn`.\n",
    "\n",
    "**Hint:** Since we have included the bias column in our design matrix explicitly, we need to adjust the `fit_intercept` parameter appropriately when creating the `LinearRegression` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sklearn_model = ...\n",
    "...\n",
    "print(\"sklearn with bias column thetas:\")\n",
    "print(sklearn_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    " \n",
    "---\n",
    " \n",
    "### Question 2c: `sklearn` and `fit_intercept`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To avoid always explicitly building in a bias column into our design matrix, `sklearn`'s `LinearRegression` object also supports `fit_intercept=True` during instantiation. \n",
    "\n",
    "Fill in the code below by first assigning `one_hot_X_nobias` to the `one_hot_X` design matrix with the bias column dropped, then fit a new `LinearRegression` model, with intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_hot_X_nobias = ...\n",
    "\n",
    "sklearn_model_intercept = ...\n",
    "...\n",
    "\n",
    "# Note that sklearn returns intercept (theta_0) and coefficients (other thetas) separately.\n",
    "# We concatenate the intercept and other thetas before printing for easier comparison with the models above.\n",
    "print(\"sklearn with intercept thetas:\")\n",
    "print(np.concatenate(([sklearn_model_intercept.intercept_], sklearn_model_intercept.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We printed the MSE for the `scipy` and both `sklearn` solutions below (all using L2 loss). Notice that while the theta coefficients are different for the two `sklearn` models (with the bias column, vs. with `fit_intercept=True`), all three models have similar MSEs! We will explain this when we explore Gradient Descent later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE scipy: \\t\\t\\t\" + str(mean_squared_error(model.predict(one_hot_X), tips)))\n",
    "print(\"MSE sklearn bias column: \\t\" + str(mean_squared_error(sklearn_model.predict(one_hot_X), tips)))\n",
    "print(\"MSE sklearn intercept model: \\t\" + str(mean_squared_error(sklearn_model_intercept.predict(one_hot_X_nobias), tips)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Fitting the Model Using Analytic Methods\n",
    "\n",
    "Let's also fit our model analytically for the L2 loss function. Recall from lecture that with a linear model, we are solving the following optimization problem for least squares:\n",
    "\n",
    "$$\\min_{\\theta} \\frac{1}{n}||\\Bbb{Y} - \\Bbb{X}\\theta||^2$$\n",
    "\n",
    "We showed in lecture that the optimal $\\hat{\\theta}$ when $\\Bbb{X}^T\\Bbb{X}$ is invertible is given by the equation: $(\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T\\Bbb{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3a: Analytic Solution Using Explicit Inverses\n",
    "\n",
    "For this problem, implement the analytic solution above using `np.linalg.inv` to compute the inverse of $\\Bbb{X}^T\\Bbb{X}$. As in `q2a`, we provide a class `MyAnalyticallyFitOLSModel` with a `fit` method to wrap this functionality.\n",
    "\n",
    "**Hint**: To compute the transpose of a matrix, you can use `X.T` or `X.transpose()`.\n",
    "\n",
    "**Note**: Make sure that `_thetas` is always a `np.ndarray` object (a NumPy array), even if `Y` is a `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyAnalyticallyFitOLSModel():\n",
    "    def __init__(self):\n",
    "        self._thetas = None\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Sets _thetas using the analytical solution to the OLS problem.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X: A 2D DataFrame (or NumPy array) of numeric features (one-hot encoded).\n",
    "        Y: A 1D NumPy array or Series of the dependent variable.\n",
    "        \n",
    "        Returns\n",
    "        -----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        ...\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self._thetas        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to find the analytical solution for the `tips` dataset. Depending on the machine that you run your code on, **you should either see a singular matrix error or end up with some theta values that are nonsensical (magnitudes greater than $10^{15}$).** This is not good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed.\n",
    "# The try-except block suppresses errors during submission\n",
    "import traceback\n",
    "try:\n",
    "    model_analytical = MyAnalyticallyFitOLSModel()\n",
    "    model_analytical.fit(one_hot_X, tips)\n",
    "    analytical_thetas = model_analytical._thetas\n",
    "    print(analytical_thetas)\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the cell below, explain why we got the error or nonsensical theta values above when trying to calculate the analytical solution for our one-hot encoded `tips` dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3c: Fixing Our One-Hot Encoding\n",
    "\n",
    "Now, let's modify our one-hot encoding approach from earlier so we don't get the error we saw in the previous part. Complete the code below to one-hot-encode our dataset such that `one_hot_X_revised` has no redundant features. \n",
    "\n",
    "**Hint**: To identify redundancies in one-hot-encoded features, consider the number of boolean values that are required to uniquely express each possible option. For example, we only need one column to express whether an individual it's Lunch or Dinner time: If the value is 0 in the Lunch column, it tells us it must be Dinner time.\n",
    "\n",
    "**Note**: If you are running into a `matrices are not aligned` error, make sure to read the note in question 3a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_revised(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded DataFrame of our input data, removing redundancies.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: A DataFrame that may include non-numerical features.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded DataFrame that only contains numeric features without any redundancies.\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return data\n",
    "\n",
    "one_hot_X_revised = one_hot_encode_revised(X)\n",
    "display(one_hot_X_revised.head())\n",
    "    \n",
    "scipy_model = MyScipyLinearModel()\n",
    "scipy_model.fit(l2, one_hot_X_revised, tips)\n",
    "    \n",
    "analytical_model = MyAnalyticallyFitOLSModel()\n",
    "analytical_model.fit(one_hot_X_revised, tips)\n",
    "\n",
    "print(\"Our scipy numerical model's loss is: \", mean_squared_error(scipy_model.predict(one_hot_X_revised), tips))\n",
    "print(\"Our analytical model's loss is: \", mean_squared_error(analytical_model.predict(one_hot_X_revised), tips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the rank of the matrix using the `NumPy` function `np.linalg.matrix_rank`. We have printed the rank of the data and number of columns for you below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"one_hot_X: \\n\"\n",
    "      + \"\\t number of columns: \" + str(len(one_hot_X.columns)) \\\n",
    "      + \"\\trank: \" + str(np.linalg.matrix_rank(one_hot_X)))\n",
    "print(\"one_hot_X_revised: \\n\"\n",
    "      + \"\\t number of columns: \" + str(len(one_hot_X_revised.columns)) \\\n",
    "      + \"\\trank: \" + str(np.linalg.matrix_rank(one_hot_X_revised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3d: Analyzing our new One-Hot Encoding\n",
    "\n",
    "Why did removing redundancies in our one-hot encoding fix the problem we had in 3a?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "Having linearly dependent columns means our system of linear equations is underdetermined, i.e. there are multiple solutions. There is no unique solution, hence there is no inverse. An alternate approach is to use `np.linalg.pinv` or `np.linalg.solve` instead of `np.linalg.inv`. They return **a** solution among the many possible solutions. Even with the redundant features, `np.linalg.solve` `np.linalg.pinv`  will work for the example above as a result. However, in general, it's best to drop redundant features for inference purposes and we will explore this further in Homework 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "You may be wondering why `scipy.optimize.minimize` or `sklearn.LinearRegression` works as well with redundant features when an analytical solution fails above. This is because `scipy.optimize.minimize` and `sklearn.LinearRegression` use numerical optimization techniques to find **a** solution---it does not require a unique solution through matrix inverses. \n",
    "\n",
    "Consider the models we crafted in `q2b` and `q2c`: even though we fit exactly the same model (calling `.fit` with `fit_intercept=False` on data with a bias column is equivalent to calling `.fit` with `fit_intercept=True`) and achieved the same minimum MSE, the models arrived at very different coefficients. Below, we explore a numerical optimization method called **gradient descent**. It is a simpler version of the default solver used by `scipy.optimize.minimize` and `sklearn.LinearRegression`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Sinusoidal Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load the data for this problem; no further action is needed.\n",
    "df = pd.read_csv(\"data/lab8_data.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot this data, we see that there is a clear sinusoidal relationship between `x` and `y`. Here we use `plotly` to plot a scatterplot showing the relationship between `x` and `y`. If you hover over points, you will be able to see the details of the data that's attributed to a data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot the data; no further action is needed.\n",
    "import plotly.express as px\n",
    "px.scatter(df, x=\"x\", y=\"y\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll show gradient descent is so powerful it can even optimize a non-linear model (when the analytical solution is hard to derive). Specifically, we're going to model the relationship of our data by:\n",
    "\n",
    "$$\\Large{\n",
    "\\hat{y}_\\theta(x) = \\theta_1x + \\sin(\\theta_2x)\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is parameterized by $\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$.\n",
    "\n",
    "Note that a general `sin` function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$, and phase shifting parameter $c$. \n",
    "Here, we're assuming the amplitude $a$ is around 1, and the phase shifting parameter $c$ is around zero. We do not attempt to justify this assumption and you're welcome to see what happens if you ignore this assumption at the end of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We define the `sin_model` function below that outputs predictions $\\hat{\\mathbb{Y}}$ for the $y$-values $\\mathbb{Y}$ using $\\mathbb{X}$ (the $x$-values) based on our new equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to define the model; no further action is needed.\n",
    "def sin_model(X, theta):\n",
    "    \"\"\"\n",
    "    Outputs predictions (Y-hat) given X, theta_1, theta_2.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- The vector of values x.\n",
    "    theta -- A vector of length 2, where theta[0] = theta_1 and theta[1] = theta_2.\n",
    "    \"\"\"\n",
    "    theta_1 = theta[0]\n",
    "    theta_2 = theta[1]\n",
    "    return theta_1 * X + np.sin(theta_2 * X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might ask - Why we don't just represent this as a linear model with a sinusoidal feature, just like we did earlier? The issue is that the theta is INSIDE the `sin` function, and hence this formulation is **not linear in theta**. In other words, linear models use their parameters to adjust the scale of each feature, but $\\theta_2$ in this model adjusts the frequency of the feature. There are tricks we could play to use our linear model framework here, but we won't attempt this in our lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology: Loss\n",
    "\n",
    "We use the word \"loss\" in two different (but very related) contexts in this course.\n",
    "* In general, the loss is the cost function that measures how far off a model's prediction(s) is(are) from the actual value(s).\n",
    "    * **Per-datapoint loss** is a cost function that measures the cost of $y$ vs $\\hat{y}$ for a particular datapoint. For example, $L(y, \\hat{y}) = (y - \\hat{y})^2$ is the L2 loss of the observed and predicted pair ($y$, $\\hat{y}$).\n",
    "    * **Loss** (without any adjectives) is generally a cost function measured across all data points. We often use this term interchangeably with **empirical risk** to denote the average per-datapoint loss. For example, MSE can be denoted as $MSE(\\theta) = L(\\theta) = R(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$\n",
    "* If we're being particular about notation, we usually prioritize using the latter definition, because we don't particularly look at a given data point's loss when optimizing a model. In other words, the dataset-level loss is the **objective function** that we'd like to minimize.\n",
    "    * Example: \"gradient of L2 loss\" means gradient of Mean Squared Error, not per-datapoint L2 loss.\n",
    "* In this particular lab, however, we'll stick to using $L(\\theta)$ for consistency with the lecture. In future work, we will be clearer about the distinction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 4a: Computing the Gradient of the MSE With Respect to Theta on the Sine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Recall $\\hat{\\theta}$ is the value of $\\theta$ that minimizes our loss function. One way to solve for $\\hat{\\theta}$ is by computing the gradient of our loss function with respect to $\\theta$, like we did in Lecture 13. Recall that the gradient is a column vector of partial derivatives (one partial derivative per parameter).\n",
    "\n",
    "**Task**: Write/derive the expressions for the following values and use them to fill in the functions below. Working out the partial derivatives on paper is a great starting point for implementing `sin_MSE_dt1` and `sin_MSE_dt2`!\n",
    "\n",
    "* $L(\\theta)$ as `sin_mse`: Our cost function, mean squared error, where `theta` represents $\\theta = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$. \n",
    "Recall that $$L(\\vec{\\theta}, \\mathbb{X}, \\mathbb{Y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2.$$\n",
    "* $\\frac{\\partial L }{\\partial \\theta_1}$ as `sin_MSE_dt1`: The partial derivative of $L$ with respect to $\\theta_1$.\n",
    "* $\\frac{\\partial L }{\\partial \\theta_2}$ as `sin_MSE_dt2`: The partial derivative of $L$ with respect to $\\theta_2$.\n",
    "* We have completed for you `sin_MSE_gradient`, which computes $\\nabla_{\\vec{\\theta}} L(\\vec{\\theta}, \\mathbb{X}, \\mathbb{Y})$  by calling `dt1` and `dt2` and returns the gradient `dt`.\n",
    "\n",
    "**Notes:**\n",
    "* We are still working with the `DataFrame` `df`.\n",
    "* To keep our code more concise, use `np.mean` instead of taking `np.sum` and then dividing by the length of the `NumPy` array.\n",
    "* Another way to keep your code more concise is to use the function `sin_model` we defined which computes the output of the model.\n",
    "* Review the \"Terminology: Loss\" section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sin_MSE(theta, X, Y):\n",
    "    \"\"\"\n",
    "    Returns the numerical value of the l2 loss of our sinusoidal model given theta.\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- The vector of values theta.\n",
    "    X     -- The vector of x values - note that sin_model only needs a vector of X values and handles the transformations.\n",
    "    Y     -- The vector of y values.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "def sin_MSE_dt1(theta, X, Y):\n",
    "    \"\"\"\n",
    "    Returns the numerical value of the partial derivative of l2 loss with respect to theta_1.\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- The vector of values theta.\n",
    "    X     -- The vector of x values.\n",
    "    Y     -- The vector of y values.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "def sin_MSE_dt2(theta, X, Y):\n",
    "    \"\"\"\n",
    "    Returns the numerical value of the partial of l2 loss with respect to theta_2.\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- The vector of values theta.\n",
    "    X     -- The vector of x values.\n",
    "    Y     -- The vector of y values.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "# This function calls dt1 and dt2 and returns the gradient dt.\n",
    "# It is already implemented for you.\n",
    "def sin_MSE_gradient(theta, X, Y):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta.\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- The vector of values theta.\n",
    "    X     -- The vector of x values.\n",
    "    Y     -- The vector of y values.\n",
    "    \"\"\"\n",
    "    return np.array([sin_MSE_dt1(theta, X, Y), sin_MSE_dt2(theta, X, Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 4b: Implementing Gradient Descent and Using It to Optimize the Sine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's now implement gradient descent. \n",
    "\n",
    "Recall that the gradient descent update function follows the form:\n",
    "\n",
    "$$\\large\n",
    "\\vec{\\theta}^{(t+1)} \\leftarrow \\vec{\\theta}^{(t)} - \\alpha \\left (\\nabla_{\\vec{\\theta}} L(\\vec{\\theta}, \\mathbb{X}, \\mathbb{Y}) \\right )\n",
    "$$\n",
    "where \n",
    "\n",
    "- $\\vec{\\theta}^{(t+1)}$: Our new coefficients after making an update.\n",
    "- $\\vec{\\theta}^{(t)}$: Our current coefficients.\n",
    "- $\\alpha$: Our step size or learning rate.\n",
    "- $\\nabla_{\\vec{\\theta}} L(\\vec{\\theta}, \\mathbb{X}, \\mathbb{Y})$: The gradient of our loss.\n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Save the current `theta` in `theta_history`, along with the average squared loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "After completing the function, the cell will output the trajectory from running gradient descent over time.\n",
    "\n",
    "\n",
    "Note that the function you're implementing here is somewhat different than the gradient descent function we created in the lecture. The version in the lecture was `gradient_descent(df, initial_guess, alpha, n)`, where `df` was the gradient of the function we are minimizing and `initial_guess` are the starting parameters for that function. Here our signature is a bit different (described below in docstring) than the `gradient_descent` implementation from the lecture.\n",
    "\n",
    "**Hints:**\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times. Take a look at `num_iter`.\n",
    "- Be sure to include the initial theta and loss into the trajectory because the test checks for this.\n",
    "- Don't forget that `sin_MSE` and `sin_MSE_gradient` require the $x$ and $y$ values to be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_theta():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent.\"\"\"\n",
    "    return np.array([0, 0])\n",
    "\n",
    "def grad_desc(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate.\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- The loss function to be minimized (used for computing loss_history).\n",
    "    gradient_loss_f -- The gradient of the loss function to be minimized.\n",
    "    theta -- The vector of values theta to use at the first iteration.\n",
    "    data -- The data used in the model. \n",
    "    num_iter -- The max number of iterations.\n",
    "    alpha -- The learning rate (also called the step size).\n",
    "    \n",
    "    Return:\n",
    "    theta -- The optimal value of theta after num_iter of gradient descent.\n",
    "    theta_history -- The list of theta values over each iteration of gradient descent.\n",
    "    loss_history -- The list of loss values over each iteration of gradient descent.\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "\n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat, thetas_used, losses_calculated = grad_desc(\n",
    "    sin_MSE, sin_MSE_gradient, theta_start, df, num_iter=20, alpha=0.1\n",
    ")\n",
    "for b, l in zip(thetas_used, losses_calculated):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect our results of running gradient descent to optimize $\\vec{\\theta}$. The code below plots our $x$-values with our model's predicted $\\hat{y}$-values over the original scatter plot. You should notice that gradient descent successfully optimized $\\vec{\\theta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = init_theta()\n",
    "theta_est, thetas, loss = grad_desc(sin_MSE, sin_MSE_gradient, theta_init, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting our model output over our observations shows that gradient descent did  a great job finding both the overall increase (slope) of the data, as well as the oscillation frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df['x'], df['y']\n",
    "Y_pred = sin_model(X, theta_est)\n",
    "\n",
    "plt.plot(X, Y_pred, label='Model ($\\hat{y}$)')\n",
    "plt.scatter(X, Y, alpha=0.5, label='Observation ($y$)', color='gold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Loss\n",
    "\n",
    "Let's visualize our loss functions and gain some insight as to how gradient descent optimizes our model parameters.\n",
    "\n",
    "In the previous plot, we saw the loss decrease with each iteration. In this part, we'll see the trajectory of the algorithm as it travels the loss surface. Run the following cells to see a visualization of this trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.array(thetas).squeeze()\n",
    "loss = np.array(loss)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see a 3D plot (gradient descent with static alpha).\n",
    "from lab8_utils import plot_3d\n",
    "plot_3d(thetas[:, 0], thetas[:, 1], loss, mean_squared_error, sin_model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, X, Y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: An (N, 2) array of theta history.\n",
    "        loss: A list or array of loss values.\n",
    "        loss_function: For example, l2_loss.\n",
    "        model: For example, sin_model.\n",
    "        X: The original X input.\n",
    "        Y: The original Y output.\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # A list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # A list or array of theta_2 value\n",
    "\n",
    "    ## In the following block of code, we generate the z-value\n",
    "    ## across a 2D grid\n",
    "    theta1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    theta2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(theta1_s, theta2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for theta1, theta2 in data:\n",
    "        l = loss_function(model(X, np.array([theta1, theta2])), Y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "\n",
    "    fig1, ax = plt.subplots(figsize=(30, 10))\n",
    "    cs = ax.contourf(x_s, y_s, z, levels=20, cmap='viridis')\n",
    "    ax.scatter(theta_1_series, theta_2_series, c='w')\n",
    "    ax.plot(theta_1_series, theta_2_series, c='w')\n",
    "\n",
    "    cbar = fig1.colorbar(cs)\n",
    "\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot('Gradient Descent with Static Learning Rate', thetas, mean_squared_error, sin_model, df[\"x\"], df[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, gradient descent is able to navigate even this fairly complex loss space and find a nice minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "To save some computational time, you will often see two modifications in the standard library:\n",
    "\n",
    "* Instead of always making some fixed number of iterations, we can terminate early if we determine that `theta` already converges. Convergence is usually defined by $\\theta_{t+1} - \\theta_{t}$ is less than some number, and you can specify this in the `tol` argument in `scipy.optimize.minimize` function. The maximum iteration can be set using the `maxiter` parameter as well.\n",
    "* Instead of calculating the risk by averaging over all data points, we estimate the risk by averaging over some subsample of data points. This is known as stochastic gradient descent and is usually denoted as SGD. See the relevant lecture slides for a more precise definition of SGD in the context of Data 100.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Ruby congratulates you for finishing Lab 08!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ruby.jpeg' width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://forms.gle/owfPCGgnrju1xQEA9). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Submit this file to the Lab 08 assignment on Gradescope. Gradescope will automatically submit the PDF from this file to the Lab 08 Written assignment. There is no need to manually submit Lab 08 Written answers; however, please check that the PDF was generated and submitted correctly. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> one_hot_X.shape == (244, 13)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all([np.issubdtype(one_hot_X[column].dtype, np.number) for column in one_hot_X])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(model._thetas) == 13\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(model_l1._thetas) == 13\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_model = MyScipyLinearModel()\n>>> test_model.fit(l1, one_hot_X, tips)\n>>> len(model_l1.predict(one_hot_X)) == 244\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_model = MyScipyLinearModel()\n>>> test_model.fit(l2, one_hot_X, tips)\n>>> len(test_model.predict(one_hot_X)) == 244\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(sklearn_model, LinearRegression)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(sklearn_model.coef_) == 13\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> len(sklearn_model.predict(one_hot_X)) == 244\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(sklearn_model_intercept.coef_) == 12\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sklearn_model_intercept.intercept_ != 0\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_test = np.array([[1, 2], [-1, -9]])\n>>> Y_test = np.array([1, 3])\n>>> model_test = MyAnalyticallyFitOLSModel()\n>>> model_test.fit(X_test, Y_test)\n>>> assert np.all(np.isclose(model_test._thetas, np.array([2.14285714, -0.57142857])))\n>>> assert len(model_test.predict(X_test)) == 2\n>>> assert type(model_test._thetas) == np.ndarray\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_test = pd.DataFrame([[1, 2], [-1, -9]])\n>>> Y_test = pd.Series([1, 3])\n>>> model_test = MyAnalyticallyFitOLSModel()\n>>> model_test.fit(X_test, Y_test)\n>>> assert np.all(np.isclose(model_test._thetas, np.array([2.14285714, -0.57142857])))\n>>> assert len(model_test.predict(X_test)) == 2\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> one_hot_X_revised.shape == (244, 9)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_squared_error(scipy_model.predict(one_hot_X_revised), tips), 1.043942840612841, rtol=1)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_squared_error(analytical_model.predict(one_hot_X_revised), tips), 1.043942840612841, rtol=1)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X, Y = (df['x'], df['y'])\n>>> np.isclose(sin_MSE([0, np.pi], X, Y), 19.49000412080223)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X, Y = (df['x'], df['y'])\n>>> np.isclose(sin_MSE([0, np.pi / 2], X, Y), 20.954427404991762)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X, Y = (df['x'], df['y'])\n>>> np.isclose(sin_MSE_dt1([0, np.pi], X, Y), -25.37666067092453)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X, Y = (df['x'], df['y'])\n>>> np.isclose(sin_MSE_dt1([0, np.pi / 2], X, Y), -25.815630534245813)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X, Y = (df['x'], df['y'])\n>>> np.isclose(sin_MSE_dt2([0, np.pi], X, Y), 1.9427210155296564)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X, Y = (df['x'], df['y'])\n>>> np.isclose(sin_MSE_dt2([0, np.pi / 2], X, Y), -8.680852400281287)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(np.round(theta_hat[0], 8), 1.51625373) and np.isclose(np.round(theta_hat[1], 8), 2.99448441)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(np.round(thetas_used[1][0], 8), 2.60105745) and np.isclose(np.round(thetas_used[1][1], 8), 2.60105745)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(np.round(thetas_used[3][0], 8), 2.05633644) and np.isclose(np.round(thetas_used[3][1], 7), 2.9631291)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(np.round(losses_calculated[5], 8), 0.90732714) and np.isclose(np.round(losses_calculated[8], 8), 0.29697507)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
