{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab09.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 09: Model Selection, Regularization, and Cross-Validation\n",
    "## Due Sunday, July 21, 11:59 PM PT\n",
    "\n",
    "In this lab, you will practice using `scikit-learn` to generate models of various complexity levels. You'll then use validation sets and K-fold cross-validation to select the models that generalize the best.\n",
    "\n",
    "To receive credit for a lab, answer all questions correctly and submit before the deadline.\n",
    "\n",
    "**The on-time deadline is Sunday, July 21, 11:59 PM PT**. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted. While course staff is happy to help you if you encounter difficulties with submission, we may not be able to respond to late-night requests for assistance (TAs need to sleep, after all!). **We strongly encourage you to plan to submit your work to Gradescope several hours before the stated deadline.** This way, you will have ample time to contact staff for submission support.\n",
    "\n",
    "**This lab contains four written questions**, which will be graded based on completion and coherence. After submitting this assignment to the Lab 09 Coding assignment on Gradescope, Gradescope will automatically submit the PDF from this file to the Lab 09 Written assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about this assignment, we ask that you **write your solutions individually**. If you discuss the assignment with others, please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "**Note:** Some part of the video is recorded in Spring 2022. There may be slight inconsistencies between the version you are viewing and the version used in the recording, but content is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"uQ3E4pejmD8\", list = 'PLQCcNQgUcDfqlXYhOXB-ZiE9V-4v-DU6I', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook; no further action is needed.\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "from IPython.display import display, Latex, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "### Introduction\n",
    "\n",
    "For this lab, we will use a toy dataset to predict the house prices in Boston with data provided by the `sklearn.datasets` package. There are more interesting datasets in the package if you want to explore them during your free time!\n",
    "\n",
    "Run the following cell to load the data. `pickle.load()` will return a dictionary object which includes keys for:\n",
    "- `data` : The independent variables/features (X),\n",
    "- `target` : The response vector (Y),\n",
    "- `feature_names`: The column names,\n",
    "- `DESCR` : A full description of the data, and\n",
    "- `filename`: The name of the CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "boston_data = pickle.load(open(\"boston_data.pickle\", \"rb\")) \n",
    "\n",
    "print(\"Dictionary keys:\")\n",
    "print(boston_data.keys())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Sum of the attributes:\")\n",
    "print(sum(boston_data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "data_description",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "A look at the `DESCR` attribute tells us the data contains these features:\n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    2. ZN        proportion of residential land zoned for lots over \n",
    "                 25,000 sq.ft.\n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds \n",
    "                 river; 0 otherwise)\n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    6. RM        average number of rooms per dwelling\n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    10. TAX      full-value property-tax rate per 10,000 USD\n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    12. LSTAT    % lower status of the population\n",
    "    \n",
    "Let's now convert this data into a `pandas` `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "data_head",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(boston_data['data'], columns=boston_data['feature_names'])\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Let's model this housing price data! Before we can do this, however, we need to split the data into training and validation sets. Remember that the response vector (housing prices) lives in the `target` attribute. A random seed is set here so we can deterministically generate the same splitting in the future if we want to test our result again and find potential bugs.\n",
    "\n",
    "Use the sklearn's `train_test_split` [(documentation)](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split out 10% of the data for the validation set. Call the resulting splits `X_train`, `X_validation`, `Y_train`, and `Y_validation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(45) # DO NOT CHANGE\n",
    "\n",
    "X = boston\n",
    "Y = pd.Series(boston_data['target'])\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "As a warmup, fit a linear model to describe the relationship between the housing price and all available independent variables/features. We have imported `sklearn.linear_model` as `lm`, so you can use that instead of typing out the whole module name. Fill in the cell below to fit a model using the data contained in the training set. Then, use this fitted model to predict the housing prices for the validation set. We've written out the code to create a scatter plot showing the relationship between the predicted and actual housing prices in the validation set for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "linear_model = lm.LinearRegression()\n",
    "\n",
    "# Fit your linear model\n",
    "# linear_model.fit(...)\n",
    "\n",
    "# Predict housing prices on the validation set\n",
    "Y_pred = ...\n",
    "\n",
    "# DO NOT CHANGE THE CODE BELOW THIS LINE\n",
    "# Plot true prices vs. predicted values\n",
    "plt.scatter(Y_validation, Y_pred, alpha=0.5)\n",
    "\n",
    "# Plot the x=y diagonal line\n",
    "plt.plot([0, 50], [0, 50], color='r')\n",
    "plt.xlabel(\"Prices $(y)$\")\n",
    "plt.ylabel(\"Predicted Prices $(\\hat{y})$\")\n",
    "plt.title(\"Prices vs. Predicted Prices\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "Briefly analyze the scatter plot above. Do you notice any outliers? Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "Alternatively, we can plot the residuals against our model predictions (fitted values). This is known as the **residual plot**. Ideally, they would all be zero. Given the inevitability of noise, we would at least like them to be scattered randomly across the line where the residual is zero. By contrast, there appears to be a possible pattern with our model consistently underestimating prices for both very low and very high values, and possibly consistently overestimating prices towards the middle range below. We can figure out whether the model is overestimating or underestimating based on the sign of the residual: if it is negative, it means $y_i < \\hat{y}_i$ so we are overestimating prices. Conversely, if the residual is positive, $y_i > \\hat{y_i}$ and we end up underestimating prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_pred, Y_validation - Y_pred, alpha=0.5)\n",
    "plt.ylabel(\"Residual $(y - \\hat{y})$\")\n",
    "plt.xlabel(\"Predicted Prices $(\\hat{y})$\")\n",
    "plt.title(\"Residuals vs. Predicted Prices\")\n",
    "plt.title(\"Residual of prediction for i-th house\")\n",
    "plt.axhline(y = 0, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "As we see in the scatter plots above, our model is not perfect. If it were perfect, the first scatter plot of predicted vs. true prices would follow the identity line (i.e., a line of slope 1 or $\\hat{y} = y$). In contrast, the second scatter plot of residuals vs. predicted prices would be scattered randomly around the line where the residual is 0. To quantify the performance of the model, we will compute the Root Mean Squared Error (RMSE) of the predicted responses: \n",
    "\n",
    "$$\n",
    "\\textbf{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 }\n",
    "$$\n",
    "\n",
    "Fill in the function below to compute the RMSE. Then, assign `train_error` to the model’s RMSE when making predictions on the training data and `validation_error` to the model’s RMSE when making predictions on the validation set. Your implementation **should not** use `for` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "def rmse(actual_y, predicted_y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        predicted_y: An array of the predictions from the model.\n",
    "        actual_y: An array of the ground truth labels.\n",
    "        \n",
    "    Returns:\n",
    "        The root mean square error between the predictions and ground truth labels.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "train_error = ...\n",
    "validation_error = ...\n",
    "\n",
    "print(\"Training RMSE:\", train_error)\n",
    "print(\"Validation RMSE:\", validation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Is your training error lower than the error on the validation set, which includes data the model never got to see while it was being trained? If so, why could this be happening? Answer in the cell below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "Sometimes we can get even higher accuracy by adding more features. For example, the code below adds the square, square root, and hyperbolic tangent of every feature to the design matrix. We've chosen these bizarre features specifically to highlight overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_with_extra_features = boston.copy()\n",
    "for feature_name in boston.columns:\n",
    "    boston_with_extra_features[feature_name + \"^2\"] = boston_with_extra_features[feature_name] ** 2\n",
    "    boston_with_extra_features[\"sqrt\" + feature_name] = np.sqrt(boston_with_extra_features[feature_name])\n",
    "    boston_with_extra_features[\"tanh\" + feature_name] = np.tanh(boston_with_extra_features[feature_name])\n",
    "    \n",
    "boston_with_extra_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split up our data again and refit the model. From this cell forward, we append `2` to the variable names `X_train, X_validation, Y_train, Y_validation, train_error, validation_error` in order to maintain our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(25)\n",
    "X = boston_with_extra_features\n",
    "X_train2, X_validation2, Y_train2, Y_validation2 = train_test_split(X, Y, test_size = 0.1)\n",
    "linear_model.fit(X_train2, Y_train2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the training and validation RMSE values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_error2 = rmse(Y_train2, linear_model.predict(X_train2)) \n",
    "validation_error2 = rmse(Y_validation2, linear_model.predict(X_validation2))\n",
    "\n",
    "print(\"Training RMSE:\", train_error2)\n",
    "print(\"Validation RMSE:\", validation_error2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our training and validation RMSE, we see that they are lower than you computed earlier. This strange model is seemingly better even though it includes seemingly useless features like the hyperbolic tangent of the average number of rooms per dwelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates the training and validation RMSE for 49 different models and stores the results in a `DataFrame`. The first model uses only the first feature \"CRIM\". The second model uses the first two features \"CRIM\" and \"ZN\", and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_vs_N = pd.DataFrame(columns = [\"N\", \"Training Error\", \"Validation Error\"])\n",
    "\n",
    "range_of_num_features = range(1, X_train2.shape[1] + 1)\n",
    "\n",
    "# Iterates through different number of features\n",
    "for N in range_of_num_features:\n",
    "    # Use only the first N features for this model\n",
    "    X_train_first_N_features = X_train2.iloc[:, :N]    \n",
    "    linear_model.fit(X_train_first_N_features, Y_train2)\n",
    "    train_error_overfit = rmse(Y_train2, linear_model.predict(X_train_first_N_features))\n",
    "    \n",
    "    # Preprocess validation set the same way as the training data\n",
    "    X_validation_first_N_features = X_validation2.iloc[:, :N]\n",
    "    validation_error_overfit = rmse(Y_validation2, linear_model.predict(X_validation_first_N_features))    \n",
    "    \n",
    "    # Save the RMSE\n",
    "    errors_vs_N.loc[len(errors_vs_N)] = [N, train_error_overfit, validation_error_overfit]\n",
    "\n",
    "errors_vs_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the training and validation error as we add each additional feature, our training error gets lower and lower, and in fact, it's possible to prove with linear algebra that the training error will decrease monotonically.\n",
    "\n",
    "By contrast, the error on unseen validation data is higher for the models with more parameters, since the lessons learned from these last 20+ features aren't actually useful when applied to unseen data. That is, these models aren't generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.line(errors_vs_N, x = \"N\", y = [\"Training Error\", \"Validation Error\"], width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "This plot is a useful tool for **model selection**. Describe your observation in the plot above. Can this plot be used to inform the number of features you should use for modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "**Bonus:** You may be tempted to iterate through all possible feature combinations. While this may be possible when the dataset contains very few features, a dataset with 48 features has a total of ${48 \\choose 1} + {48 \\choose 2} + {48 \\choose 3} + \\cdots + {48 \\choose 48}$ combinations. In practice, we often use heuristics like the example above that perform a reasonable amount of comparisons instead of trying out every combination due to computational constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is the formal term that describes the process of limiting a model’s complexity, often with the aim of reducing overfitting and allowing for more generalizable models. Here, we consider Ridge Regression, also termed L2 Regularization (in contrast with LASSO Regression or L1 Regularization), as discussed in Lecture 15. Mathematically speaking, our objective function looks fairly similar to the usual formula for MSE with the addition of the regularization term: \n",
    "\n",
    "$$\\min_{\\theta} \\frac{1}{n} || \\mathbb{Y} - \\mathbb{X}\\theta ||^{2} + \\lambda \\sum_{j=1}^{d} \\theta_j^{2}$$\n",
    "\n",
    "As an alternative and more realistic example, instead of using only the first N features, we can use various different regularization strengths. For example, for really low regularization strengths (e.g. $\\lambda = 10^{-12}$), we get a model that is nearly identical to our linear regression model. This regularization term is so small you may even get a `LinAlgWarning`. Below, we import `Ridge`, which initializes a model similar to `lm.LinearRegression` but applies Ridge Regression.\n",
    "\n",
    "**Note**: `sklearn` uses `alpha` to represent $\\lambda$. The `alpha` parameter passed into `Ridge` represents the regularization parameter, and here, does not refer to the learning rate in the context of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "regularized_model = Ridge(alpha = 1e-12)\n",
    "regularized_model.fit(X_train2, Y_train2)\n",
    "ridge_coefs_low_regularization = regularized_model.coef_\n",
    "\n",
    "linear_model = linear_model.fit(X_train2, Y_train2)\n",
    "linear_coefs = linear_model.coef_\n",
    "\n",
    "# Most of the differences are 0's\n",
    "(ridge_coefs_low_regularization - linear_coefs).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we pick a large regularization strength, e.g. $\\lambda = 10^2$, we see that the resulting parameters are much smaller in magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_model = Ridge(alpha = 10**2)\n",
    "regularized_model.fit(X_train2, Y_train2)\n",
    "regularized_model_coefs = regularized_model.coef_\n",
    "\n",
    "# The difference in magnitude are mostly negative, \n",
    "# this shows that regularized_model has a smaller coefficient in magnitude\n",
    "(abs(regularized_model_coefs) - abs(linear_coefs)).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling / Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture that in order to properly regularize a model, the features should be at the same scale. Otherwise, the model has to spend more of its parameter budget to use \"small\" features (e.g., lengths in inches) compared to \"large\" features (e.g., lengths in kilometers).\n",
    "\n",
    "To do this we can use sklearn's `StandardScaler` [(documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to create a new version of the `DataFrame` where every column has a mean of zero and a standard deviation of 1. \n",
    "\n",
    "**Optional:** Can you implement this using a `pandas` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(boston_with_extra_features)\n",
    "boston_with_extra_features_scaled = pd.DataFrame(ss.transform(boston_with_extra_features), \\\n",
    "                                                 columns = boston_with_extra_features.columns)\n",
    "boston_with_extra_features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now regenerate the training and validation sets using this new rescaled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(25)\n",
    "X = boston_with_extra_features_scaled\n",
    "X_train3, X_validation3, Y_train3, Y_validation3 = train_test_split(X, Y, test_size = 0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting our regularized model with $\\lambda = 10^2$ on this scaled data, we now see that our coefficients are of around the same magnitude. This is because all of our features are of around the same magnitude, whereas in the unscaled data, some of the features, like `TAX^2`, were much larger than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_model_standardized = Ridge(alpha = 10**2)\n",
    "regularized_model_standardized.fit(X_train3, Y_train3)\n",
    "regularized_model_standardized.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3d: Finding an Optimum $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the cell below, write code that generates a `DataFrame` with the training and validation error for the range of $\\lambda$s given. Make sure you're using the 3rd training and validation sets, which have been rescaled, that is, `X_train3`, `X_validation3`, `Y_train3`, and `Y_validation3`. To lay out the process:\n",
    "\n",
    "1. Initialize a new `lm.Ridge` model with the regularization hyperparameter set to the new value of $\\lambda$.\n",
    "2. Fit the model to the training set, `X_train3`.\n",
    "3. Compute the RMSE of the model on the training and validation sets.\n",
    "4. Add a row to the `DataFrame` containing the value of $\\lambda$, training error, and validation error.\n",
    "\n",
    "**Note**: You should use all 48 features for every single model that you fit, i.e. you're not going to be keeping only the first $N$ features.\n",
    "\n",
    "**Hint**: It is possible to \"append\" or add a row to a `DataFrame` by calling `loc`, e.g. `df.loc[len(df)] = [2, 3, 4]` (assuming `df` has 3 columns!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_vs_lambda = pd.DataFrame(columns = [\"lambda\", \"Training Error\", \"Validation Error\"])\n",
    "range_of_lambda = 10**np.linspace(-5, 4, 40)\n",
    "\n",
    "for lamb in range_of_lambda:\n",
    "    regularized_model_lambda = ...\n",
    "    ...\n",
    "    train_error_overfit = ...\n",
    "    validation_error_overfit = ...\n",
    "    error_vs_lambda.loc[len(error_vs_lambda)] = ...\n",
    "\n",
    "error_vs_lambda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot your training and validation set error for the range of $\\lambda$s given. You should see a figure where training error decreases as model complexity increases, but the error on the validation set is large for extreme values of $\\lambda$ and minimized for some intermediate values. Holding your mouse over the training error and validation error lines will display the $\\lambda$ and value at that point.\n",
    "\n",
    "Note that on your plot, the **x-axis is the inverse of complexity**! In other words, small $\\lambda$ models (on the left) are complex, because there is no regularization. That's why the training error is lowest on the left side of the plot, as this is where overfitting occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(error_vs_lambda, x = \"lambda\", y = [\"Training Error\", \"Validation Error\"], log_x=True, height = 400, width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3e\n",
    "\n",
    "Based on the plot above, which $\\lambda$ value will you use in your model? Keep in mind that the x-axis in the plot above has a logarithmic scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## REMINDER: Test Set vs. Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The validation set is sometimes referred to as a \"development set\"; these terms are exactly synonymous.\n",
    "\n",
    "In the plots above, we trained our models on a training set and plotted the resulting RMSE on the training set in blue. We also held out a set of data and plotted the error on this validation set in red, calling it the \"validation set error\".\n",
    "\n",
    "Note that, while it is true that your code never supplied `X_validation3` or `Y_validation3` to the fit function of the ridge regression models, once you decide to use the validation set to select between different models, different hyperparameters, or different sets of features, then we are not using that dataset as a \"test set\" but rather a \"validation set\". That is, since we've used this set for picking $\\lambda$, the resulting errors are no longer unbiased predictors of our performance on unseen models -- the true error on an unseen dataset is likely to be somewhat higher than the validation set. After all, we trained 40 models and picked the best one!\n",
    "In many real-world contexts, model builders will split their data into three sets: training, validation, and test sets, where the test set is only ever used once. That is, there are two holdout sets: one used as a validation/development set (for model selection), and one used as a test set (for providing an unbiased estimate of error).r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cv",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## An Alternate Strategy for Hyper Parameter Selection: K-Fold Cross Validation\n",
    "\n",
    "Earlier, we used simple cross-validation for model selection. Another approach is K-fold cross-validation. This allows us to use more data for training instead of having to set aside some specifically for hyperparameter selection. However, doing so requires more computation resources, as we'll have to fit K models per hyperparameter choice.\n",
    "\n",
    "In our course, there's really no reason not to use cross-validation. However, in environments where models are very expensive to train (e.g. deep learning), you'll typically prefer using a single validation set (simple cross-validation) rather than K-fold cross-validation.\n",
    "\n",
    "To emphasize what K-fold cross-validation actually means, we're going to manually carry out the procedure. Recall the approach looks something like the figure below for 4-fold cross-validation:\n",
    "\n",
    "<img src=\"cv.png\" width=800px>\n",
    "\n",
    "When we use K-fold cross-validation, rather than using a validation set for model selection, we instead use the training set for model selection. To select between various features, models, or hyperparameters, we split the training set further into multiple temporary train and validation sets (each split is called a \"fold\", hence K-fold cross-validation). We will use the average validation error across all K folds to make our optimal feature, model, and hyperparameter choices. In this example, we'll only use this procedure for hyperparameter selection, specifically to choose the best $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Scikit-learn has built-in support for cross-validation. However, to better understand how cross-validation works, complete the following function which cross-validates a given model.\n",
    "\n",
    "1. Use `sklearn`'s `KFold.split` ([documentation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)) function to get 4 splits on the training data. Note that `split` returns the indices of the data for that split.\n",
    "2. For **each** split:\n",
    "    1. Select the training and validation rows and columns based on the split indices and features.\n",
    "    2. Compute the RMSE on the validation split.\n",
    "    3. Return the average error across all cross-validation splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def compute_CV_error(model, X_train, Y_train):\n",
    "    '''\n",
    "    Split the training data into 4 subsets.\n",
    "    For each subset, \n",
    "        - Fit a model holding out that subset.\n",
    "        - Compute the MSE on that subset (the validation set).\n",
    "    You should be fitting 4 models in total.\n",
    "    Return the average MSE of these 4 folds.\n",
    "\n",
    "    Args:\n",
    "        model: An sklearn model with fit and predict functions. \n",
    "        X_train (DataFrame): Training data.\n",
    "        Y_train (DataFrame): Label.\n",
    "    \n",
    "    Return:\n",
    "        The average validation MSE for the 4 splits.\n",
    "    '''\n",
    "    kf = KFold(n_splits=4)\n",
    "    validation_errors = []\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X_train):\n",
    "        # split the data\n",
    "        split_X_train, split_X_valid = ..., ...\n",
    "        split_Y_train, split_Y_valid = ..., ...\n",
    "\n",
    "        # Fit the model on the training split\n",
    "        ...\n",
    "        \n",
    "        # Compute the RMSE on the validation split\n",
    "        error = ...\n",
    "\n",
    "\n",
    "        validation_errors.append(error)\n",
    "        \n",
    "    return np.mean(validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Use `compute_CV_error` to add a new column to `error_vs_lambda` which gives the 4-fold cross-validation error for the given choices of $\\lambda$ in `range_of_lambda`. `cv_errors` should be a list or an array of cross-validation errors generated by `compute_CV_error` for each tested value of $\\lambda$. Again, use the 3rd training and validation sets `X_train3`, `X_validation3`, `Y_train3`, and `Y_validation3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_errors = []\n",
    "\n",
    "...\n",
    "\n",
    "error_vs_lambda[\"CV Error\"] = cv_errors\n",
    "error_vs_lambda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows the validation error that we computed in the previous problem as well as the 4-fold cross-validation error. Note that the cross-validation error shows a similar dependency on $\\lambda$ relative to the validation error. This is because they are both doing the same thing: trying to estimate the expected error on unseen data drawn from the distribution from which the training set was drawn. \n",
    "\n",
    "In other words, this figure compares simple cross-validation with 4-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.line(error_vs_lambda, x = \"lambda\", y = [\"Validation Error\", \"CV Error\"], \n",
    "        log_x=True, height = 400, width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Congratulations! You have finished Lab 09!\n",
    "\n",
    "<img src=\"birb.jpg\" alt=\"Paris\" class=\"center\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://forms.gle/owfPCGgnrju1xQEA9). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Submit this file to the Lab 09 Coding assignment on Gradescope. Gradescope will automatically submit the PDF from this file to the Lab 09 Written assignment. There is no need to manually submit Lab 09 Written answers; however, please check that the PDF was generated and submitted correctly. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> list(X_train.shape) == [455, 12]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(X_validation.shape) == [51, 12]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(Y_train.shape) == [455]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(Y_validation.shape) == [51]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose(linear_model.coef_[:5], np.array([-0.122803869, 0.0551930758, 0.0148687482, 2.79933235, -18.1745474]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(linear_model.intercept_, 41.55013565094232)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.allclose(Y_pred[:5], np.array([7.89551459, 22.93009987, 24.69820235, 21.30731715, 34.50370243]))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose((train_error, validation_error), (4.633297105625516, 5.68516086658394))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose(error_vs_lambda['lambda'], range_of_lambda)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> error_vs_lambda['Training Error'].iloc[-1] > error_vs_lambda['Training Error'].iloc[0]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> error_vs_lambda['Validation Error'].iloc[-1] > error_vs_lambda['Validation Error'].iloc[0]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(error_vs_lambda['Training Error'].iloc[:30] < error_vs_lambda['Validation Error'].iloc[:30])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(compute_CV_error(lm.LinearRegression(), X_train2[['TAX', 'INDUS', 'CRIM']], Y_train2), 7.823399340020609)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.allclose(error_vs_lambda['lambda'], range_of_lambda)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> error_vs_lambda['CV Error'].iloc[20] < error_vs_lambda['CV Error'].iloc[0]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> error_vs_lambda['CV Error'].iloc[20] < error_vs_lambda['CV Error'].iloc[-1]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(abs(error_vs_lambda['Validation Error'].iloc[10:30] - error_vs_lambda['CV Error'].iloc[10:30]) < 1)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
