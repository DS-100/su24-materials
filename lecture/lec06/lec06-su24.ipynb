{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Wrangling and Regex\n",
    "\n",
    "Adapted from Bella Crouch, Lisa Yan, Will Fithian, Joseph Gonzalez, Deborah Nolan, Sam Lau\n",
    "\n",
    "Updated by Maya Shen\n",
    "\n",
    "Working with text: applying string methods and regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:15.363920Z",
     "start_time": "2018-02-02T15:15:14.337886Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Vectorization Speed\n",
    "\n",
    "A slight warning...\n",
    "\n",
    "Inspired by Itamar Turner-Trauring's blog post: https://pythonspeed.com/articles/pandas-vectorization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import string\n",
    "import random as py_random\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows  = 10000\n",
    "df = pd.DataFrame({'x': np.random.rand(num_rows)*100,\n",
    "                   'y': np.random.rand(num_rows)*100,\n",
    "                  'sentence': [''.join(py_random.choices(string.ascii_uppercase + string.digits + '                    ', \n",
    "                                                    k=random.randint(100, 250))) for i in range(num_rows)]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic\n",
    "\n",
    "Let's begin with a basic operation: calculating the ratio of two numerical columns in the dataframe. We begin by just looking at how long it takes with vectorization vs without for one run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_t0_vec = timeit.default_timer()\n",
    "100 * (df[\"x\"] / df[\"y\"])\n",
    "ratio_t1_vec = timeit.default_timer()\n",
    "print(f\"Execution time: {ratio_t1_vec-ratio_t0_vec} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratio(row):\n",
    "    return 100 * (row[\"x\"] / row[\"y\"])\n",
    "\n",
    "ratio_t0_nonvec = timeit.default_timer()\n",
    "df.apply(calc_ratio, axis=1)\n",
    "ratio_t1_nonvec = timeit.default_timer()\n",
    "print(f\"Execution time: {ratio_t1_nonvec-ratio_t0_nonvec} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_ratio_speedup = (ratio_t1_nonvec-ratio_t0_nonvec) / (ratio_t1_vec-ratio_t0_vec)\n",
    "print(f\"Vectorized code is ~{round(single_ratio_speedup, 4)}x faster than the non-vectorized code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take the average time over 100 runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_vec_fn():\n",
    "    return 100 * (df[\"x\"] / df[\"y\"])\n",
    "\n",
    "def num_nonvec_fn():\n",
    "    return df.apply(calc_ratio, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_vec_execution_time = timeit.timeit(num_vec_fn, number=num_runs)\n",
    "print(f\"Total execution time for {num_runs} runs: {ratio_vec_execution_time} seconds\")\n",
    "print(f\"Average execution time per run: {ratio_vec_execution_time / num_runs} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_nonvec_execution_time = timeit.timeit(num_nonvec_fn, number=num_runs)\n",
    "print(f\"Total execution time for {num_runs} runs: {ratio_nonvec_execution_time} seconds\")\n",
    "print(f\"Average execution time per run: {ratio_nonvec_execution_time / num_runs} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_ratio_speedup = (ratio_nonvec_execution_time / num_runs) / (ratio_vec_execution_time / num_runs)\n",
    "print(f\"Vectorized code is ~{round(mult_ratio_speedup, 4)}x faster than the non-vectorized code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings\n",
    "Let's try it for strings / string operations now by getting the \"sentence\" length. Again, let's start by just looking at how long it takes with vectorization vs without for one run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_t0_vec = timeit.default_timer()\n",
    "df[\"sentence\"].str.split().apply(len)\n",
    "str_t1_vec = timeit.default_timer()\n",
    "print(f\"Execution time: {str_t1_vec-str_t0_vec} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(s):\n",
    "    return len(s.split())\n",
    "\n",
    "str_t0_nonvec = timeit.default_timer()\n",
    "df[\"sentence\"].apply(sentence_length)\n",
    "str_t1_nonvec = timeit.default_timer()\n",
    "print(f\"Execution time: {str_t1_nonvec-str_t0_nonvec} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_str_speedup = (str_t1_nonvec-str_t0_nonvec) / (str_t1_vec-str_t0_vec)\n",
    "print(f\"Vectorized code is ~{round(single_str_speedup, 4)}x faster than the non-vectorized code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take the average time over 100 runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_vec_fn():\n",
    "    return df[\"sentence\"].str.split().apply(len)\n",
    "\n",
    "def str_nonvec_fn():\n",
    "    return df[\"sentence\"].apply(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_vec_execution_time = timeit.timeit(str_vec_fn, number=num_runs)\n",
    "print(f\"Total execution time for {num_runs} runs: {str_vec_execution_time} seconds\")\n",
    "print(f\"Average execution time per run: {str_vec_execution_time / num_runs} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_nonvec_execution_time = timeit.timeit(str_nonvec_fn, number=num_runs)\n",
    "print(f\"Total execution time for {num_runs} runs: {str_nonvec_execution_time} seconds\")\n",
    "print(f\"Average execution time per run: {str_nonvec_execution_time / num_runs} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_str_speedup = (str_nonvec_execution_time / num_runs) / (str_vec_execution_time / num_runs)\n",
    "print(f\"Vectorized code is ~{round(mult_str_speedup, 4)}x faster than the non-vectorized code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_str_slowdown = (str_vec_execution_time / num_runs) / (str_nonvec_execution_time / num_runs)\n",
    "print(f\"Non-vectorized code is ~{round(mult_str_slowdown, 4)}x faster than the vectorized code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Demo 1: Canonicalizing County Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.read_csv(\"data/county_and_state.csv\")\n",
    "populations = pd.read_csv(\"data/county_and_population.csv\")\n",
    "\n",
    "# display allows us to view a DataFrame without returning it as an object\n",
    "display(states)\n",
    "display(populations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these DataFrames share a \"County\" column. Unfortunately, formatting differences mean that we can't directly merge the two DataFrames using the \"County\"s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.merge(populations, left_on=\"County\", right_on=\"County\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas String Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, we can **canonicalize** the \"County\" string data to apply a common formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_county(county_series):\n",
    "    return (county_series.str.lower()               # lowercase\n",
    "            .str.replace(' ', '')                   # remove space\n",
    "            .str.replace('&', 'and')                # replace &\n",
    "            .str.replace('.', '')                   # remove dot\n",
    "            .str.replace('county', '')              # remove \"county\"\n",
    "            .str.replace('parish', '')              # remove \"parish\" \n",
    "            )\n",
    "\n",
    "display(canonicalize_county(states[\"County\"]))\n",
    "display(canonicalize_county(populations[\"County\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[\"Canonical County\"] = canonicalize_county(states[\"County\"])\n",
    "populations[\"Canonical County\"] = canonicalize_county(populations[\"County\"])\n",
    "display(states)\n",
    "display(populations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the merge works as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.merge(populations, on=\"Canonical County\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "**Return to Lecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Demo 2: Extracting Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fname = 'data/log.txt'\n",
    "with open(log_fname, 'r') as f:\n",
    "    log_lines = f.readlines()\n",
    "log_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to extract the day, month, year, hour, minutes, seconds, and timezone. Looking at the data, we see that these items are not in a fixed position relative to the beginning of the string. That is, slicing by some fixed offset isn't going to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[0][20:31] #  20:31 were determined by trial-and-error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use the same range for the next log line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[1][20:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll need to use some more sophisticated thinking. Let's focus on only the first line of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = log_lines[0]\n",
    "first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the data inside the square brackes by splitting string at the square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pertinent = (\n",
    "    first.split(\"[\")[1] # remove everything before the first [\n",
    "    .split(']')[0] # Remove everything after the second square ]\n",
    ") # find the text enclosed in square brackets\n",
    "pertinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day, month,rest  = pertinent.split('/')       # split up the date/month/year \n",
    "\n",
    "print(\"Day:   \", day)\n",
    "print(\"Month: \", month)\n",
    "print(\"Rest:  \", rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, hour, minute, rest = rest.split(':')    # split up the hour:minute:second\n",
    "\n",
    "print(\"Year:   \", year)\n",
    "print(\"Hour:   \", hour)\n",
    "print(\"Minute: \", minute)\n",
    "print(\"Rest:   \", rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds, time_zone = rest.split(' ')          # split the timezone after the blank space\n",
    "day, month, year, hour, minute, seconds, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try doing the same thing using pandas `str` methods:\n",
    "\n",
    "Solution below.\n",
    "<details>\n",
    "    \n",
    "```python\n",
    "df = (\n",
    "    logs.str.split(\"[\")\n",
    "        .str[1]\n",
    "        .str.split(\"]\")\n",
    "        .str[0]\n",
    "        .str.split(\"/\", expand=True)\n",
    "        .rename(columns={0: \"Day\", 1: \"Month\", 2: \"Rest\"})\n",
    ")\n",
    "df = (\n",
    "    df.join(df[\"Rest\"].str.split(\":\", expand=True))\n",
    "        .drop(columns=[\"Rest\"])\n",
    "        .rename(columns={0: \"Year\", 1: \"Hour\", 2: \"Minute\", 3: \"Rest\"})\n",
    ")\n",
    "df = (\n",
    "    df.join(df[\"Rest\"].str.split(\" \", expand=True))\n",
    "        .drop(columns=[\"Rest\"])\n",
    "        .rename(columns = {0: \"Seconds\", 1: \"Timezone\"})\n",
    ")\n",
    "\n",
    "print(\"Final Dataframe\")\n",
    "display(df)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.read_csv(\"data/log.txt\", \n",
    "                sep=\"\\t\", \n",
    "                header=None)[0]\n",
    "\n",
    "print(\"Original input!\")\n",
    "display(logs)\n",
    "\n",
    "# finish me\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked and you will often see code like this in data cleaning pipelines.  \n",
    "\n",
    "However, **regular expressions** provide a faster and more expressive mechanism to extract strings that match certain patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "**Return to lecture**\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## String Extraction with Regex\n",
    "\n",
    "Python `re.findall` returns a list of all extracted matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"My social security number is 123-45-6789 bro, or actually maybe it’s 321-45-6789.\";\n",
    "\n",
    "pattern = r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\n",
    "\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now, let's see vectorized extraction in `pandas`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `.str.findall` returns a `Series` of lists of all matches in each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn = pd.DataFrame(\n",
    "    ['987-65-4321',\n",
    "     'forty',\n",
    "     '123-45-6789 bro or 321-45-6789',\n",
    "     '999-99-9999'],\n",
    "    columns=['SSN'])\n",
    "df_ssn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> Series of lists\n",
    "pattern = r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\n",
    "df_ssn['SSN'].str.findall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the last expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_ssn['SSN']\n",
    "    .str.findall(pattern)\n",
    "    .str[-1] # Get the last element from each list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "**Return to slides**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "\n",
    "## Extraction Using Regex Capture Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python function `re.findall`, in combination with parentheses returns specific substrings (i.e., **capture groups**) within each matched string, or **match**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I will meet you at 08:30:00 pm tomorrow\"\"\"       \n",
    "pattern = \".*(\\d\\d):(\\d\\d):(\\d\\d).*\"\n",
    "matches = re.findall(pattern, text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three capture groups in the first matched string\n",
    "hour, minute, second = matches[0]\n",
    "print(\"Hour:   \", hour)\n",
    "print(\"Minute: \", minute)\n",
    "print(\"Second: \", second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "In `pandas`, we can use `.str.extract` to extract each capture group of **only the first match** of each record into separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to SSNs\n",
    "df_ssn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the first match of all groups\n",
    "pattern_group_mult = r\"([0-9]{3})-([0-9]{2})-([0-9]{4})\" # 3 groups\n",
    "df_ssn['SSN'].str.extract(pattern_group_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging my code with the `str` accessors I often make a separate series varible so the python tab completion tools can find the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssns = df_ssn['SSN']\n",
    "ssns.str.extract(pattern_group_mult) # <- try shift+tab inside the parens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, `.str.extractall` extracts **all matches** of each record into separate columns. Rows are then MultiIndexed by original record index and match index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> DataFrame, one row per match\n",
    "df_ssn['SSN'].str.extractall(pattern_group_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "**Return to Slides**\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Canonicalization with Regex (sub, replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regular Python, canonicalize with `re.sub` (standing for \"substitute\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<div><td valign=\"top\">Moo</td></div>'\n",
    "pattern = r\"<[^>]+>\"\n",
    "re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "In `pandas`, canonicalize with `Series.str.replace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example dataframe of strings\n",
    "df_html = pd.DataFrame(['<div><td valign=\"top\">Moo</td></div>',\n",
    "                   '<a href=\"http://ds100.org\">Link</a>',\n",
    "                   '<b>Bold text</b>'], columns=['Html'])\n",
    "df_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series -> Series\n",
    "df_html[\"Html\"].str.replace(pattern, '', regex=True).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "**Return to lecture**\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Revisiting Text Log Processing using Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python `re` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = log_lines[0]\n",
    "display(line)\n",
    "\n",
    "pattern = r'\\[(\\d+)\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "day, month, year, hour, minute, second, time_zone = re.findall(pattern, line)[0] # get first match\n",
    "day, month, year, hour, minute, second, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pandas` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(log_lines, columns=['Log'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: `Series.str.findall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\[(\\d+)\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "df['Log'].str.findall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Option 2: `Series.str.extractall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log'].str.extractall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling either of these two DataFrames into a nice format (like below) is left as an exercise for you! You will do a related problem on the homework.\n",
    "\n",
    "\n",
    "||Day|Month|Year|Hour|Minute|Second|Time Zone|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|0|26|Jan|2014|10|47|58|-0800|\n",
    "|1|2|Feb|2005|17|23|6|-0800|\n",
    "|2|3|Feb|2006|10|18|37|-0800|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# Real World Case Study: Restaurant Data\n",
    "\n",
    "In this example, we will show how regexes can allow us to track quantitative data across categories defined by the appearance of various text fields.\n",
    "\n",
    "In this example we'll see how the presence of certain keywords can affect quantitative data:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio = pd.read_csv('data/violations.csv', header=0, names=['bid', 'date', 'desc'])\n",
    "desc = vio['desc']\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = desc.value_counts()\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of different descriptions!! Can we **canonicalize** at all? Let's explore two sets of 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hmmm...\n",
    "counts[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to cut out the extra info in square braces.\n",
    "vio['clean_desc'] = (vio['desc']\n",
    "             .str.replace(r'\\s*\\[.*\\]$', '', regex=True)\n",
    "             .str.strip()       # removes leading/trailing whitespace\n",
    "             .str.lower())\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonicalizing definitely helped\n",
    "vio['clean_desc'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['clean_desc'].value_counts().head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our research question:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Below, we use regular expressions and `df.assign()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html?highlight=assign#pandas.DataFrame.assign)) to **method chain** our creation of new boolean features, one per keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expressions to assign new features for the presence of various keywords\n",
    "# regex metacharacter | \n",
    "with_features = (vio\n",
    " .assign(is_unclean     = vio['clean_desc'].str.contains('clean|sanit'))\n",
    " .assign(is_high_risk = vio['clean_desc'].str.contains('high risk'))\n",
    " .assign(is_vermin    = vio['clean_desc'].str.contains('vermin'))\n",
    " .assign(is_surface   = vio['clean_desc'].str.contains('wall|ceiling|floor|surface'))\n",
    " .assign(is_human     = vio['clean_desc'].str.contains('hand|glove|hair|nail'))\n",
    " .assign(is_permit    = vio['clean_desc'].str.contains('permit|certif'))\n",
    ")\n",
    "with_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### EDA\n",
    "\n",
    "That's the end of our text wrangling. Now let's do some more analysis to analyze restaurant health as a function of the number of violation keywords.\n",
    "\n",
    "To do so we'll first group so that our **granularity** is one inspection for a business on particular date. This effectively counts the number of violations by keyword for a given inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features = (with_features\n",
    " .groupby(['bid', 'date'])\n",
    " .sum(numeric_only=True)\n",
    " .reset_index()\n",
    ")\n",
    "count_features.iloc[255:260, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out our new dataframe in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features[count_features['is_vermin'] > 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll reshape this \"wide\" table into a \"tidy\" table using a pandas feature called `pd.melt` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html?highlight=pd%20melt)) which we won't describe in any detail, other than that it's effectively the inverse of `pd.pivot_table`.\n",
    "\n",
    "Our **granularity** is now a violation type for a given inspection (for a business on a particular date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_type_df = pd.melt(count_features, id_vars=['bid', 'date'],\n",
    "            var_name='feature', value_name='num_vios')\n",
    "\n",
    "# show a particular inspection's results\n",
    "violation_type_df[(violation_type_df['bid'] == 489) & (violation_type_df['date'] == 20150728)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our research question:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)\n",
    "\n",
    "<br/>\n",
    "\n",
    "We have the second half of this question! Now let's **join** our table with the inspection scores, located in `inspections.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the scores\n",
    "inspection_df = pd.read_csv('data/inspections.csv',\n",
    "                  header=0,\n",
    "                  usecols=[0, 1, 2],\n",
    "                  names=['bid', 'score', 'date'])\n",
    "inspection_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the inspection scores were stored in a separate file from the violation descriptions, we notice that the **primary key** in inspections is (`bid`, `date`)! So we can reference this key in our join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join scores with the table broken down by violation type\n",
    "violation_type_and_scores = (\n",
    "    violation_type_df\n",
    "    .merge(inspection_df, on=['bid', 'date'])\n",
    ")\n",
    "violation_type_and_scores.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "Let's plot the distribution of scores, broken down by violation counts, for each inspection feature (`is_clean`, `is_high_risk`, `is_vermin`, `is_surface`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will learn this syntax next week. Focus on interpreting for now.\n",
    "sns.catplot(x='num_vios', y='score',\n",
    "               col='feature', col_wrap=2,\n",
    "               kind='box',\n",
    "               data=violation_type_and_scores);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can observe:\n",
    "* The inspection score generally goes down with increasing numbers of violations, as expected.\n",
    "* Depending on the violation keyword, inspections scores on average go down at slightly different rates.\n",
    "* For example, that if a restaurant inspection involved 2 violations with the keyword \"vermin\", the average score for that inspection would be a little bit below 80."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
