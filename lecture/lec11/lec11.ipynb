{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 11 â€“ Data 100, Summer 2024\n",
    "\n",
    "Data 100, Summer 2024\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/su24/acks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Big font helper\n",
    "def adjust_fontsize(size=None):\n",
    "    SMALL_SIZE = 8\n",
    "    MEDIUM_SIZE = 10\n",
    "    BIGGER_SIZE = 12\n",
    "    if size != None:\n",
    "        SMALL_SIZE = MEDIUM_SIZE = BIGGER_SIZE = size\n",
    "\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Simple Linear Regression Model\n",
    "\n",
    "We will first define some helper functions to calculate the regression line for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def standard_units(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "def correlation(x, y):\n",
    "    return np.mean(standard_units(x) * standard_units(y))\n",
    "\n",
    "def slope(x, y):\n",
    "    return correlation(x, y) * np.std(y) / np.std(x)\n",
    "\n",
    "def intercept(x, y):\n",
    "    return np.mean(y) - slope(x, y)*np.mean(x)\n",
    "\n",
    "def fit_least_squares(x, y):\n",
    "    theta_0 = intercept(x, y)\n",
    "    theta_1 = slope(x, y)\n",
    "    return theta_0, theta_1\n",
    "\n",
    "def predict(x, theta_0, theta_1):\n",
    "    return theta_0 + theta_1*x\n",
    "\n",
    "def compute_mse(y, yhat):\n",
    "    return np.mean((y - yhat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.style.use('default') # Revert style to default mpl\n",
    "NO_VIZ, RESID, RESID_SCATTER = range(3)\n",
    "def least_squares_evaluation(x, y, visualize=NO_VIZ):\n",
    "    # statistics\n",
    "    print(f\"x_mean : {np.mean(x):.2f}, y_mean : {np.mean(y):.2f}\")\n",
    "    print(f\"x_stdev: {np.std(x):.2f}, y_stdev: {np.std(y):.2f}\")\n",
    "    print(f\"r = Correlation(x, y): {correlation(x, y):.3f}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    ahat, bhat = fit_least_squares(x, y)\n",
    "    yhat = predict(x, ahat, bhat)\n",
    "    print(f\"\\theta_0: {ahat:.2f}, \\theta_1: {bhat:.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(compute_mse(y, yhat)):.3f}\")\n",
    "\n",
    "    # visualization\n",
    "    fig, ax_resid = None, None\n",
    "    if visualize == RESID_SCATTER:\n",
    "        fig, axs = plt.subplots(1,2,figsize=(8, 3))\n",
    "        axs[0].scatter(x, y)\n",
    "        axs[0].plot(x, yhat)\n",
    "        axs[0].set_title(\"LS fit\")\n",
    "        ax_resid = axs[1]\n",
    "    elif visualize == RESID:\n",
    "        fig = plt.figure(figsize=(4, 3))\n",
    "        ax_resid = plt.gca()\n",
    "    \n",
    "    if ax_resid is not None:\n",
    "        ax_resid.scatter(x, y - yhat, color = 'red')\n",
    "        ax_resid.plot([4, 14], [0, 0], color = 'black')\n",
    "        ax_resid.set_title(\"Residuals\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at four different datasets. Without visualizing the data, let's try fitting the simple linear regression model to these four datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in four different datasets: I, II, III, IV\n",
    "x = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\n",
    "y1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n",
    "y2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\n",
    "y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\n",
    "x4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\n",
    "y4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "\n",
    "anscombe = {\n",
    "    'I': pd.DataFrame(list(zip(x, y1)), columns =['x', 'y']),\n",
    "    'II': pd.DataFrame(list(zip(x, y2)), columns =['x', 'y']),\n",
    "    'III': pd.DataFrame(list(zip(x, y3)), columns =['x', 'y']),\n",
    "    'IV': pd.DataFrame(list(zip(x4, y4)), columns =['x', 'y'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(['I', 'II', 'III', 'IV']):\n",
    "    ans = anscombe[dataset]\n",
    "    x, y = ans[\"x\"], ans[\"y\"]\n",
    "    ahat, bhat = fit_least_squares(x, y)\n",
    "    print(f\"Dataset {dataset}: theta_0: {ahat:.2f}, theta_1: {bhat:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all four datasets have exactly the same fit. \n",
    "\n",
    "In fact, all four of them have the same $\\bar x$, $\\bar y$, $\\sigma_x$, $\\sigma_y$, correlation $r$, and RMSE! If we only look at these statistics, we will probably be inclined to say that these datasets are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['I', 'II', 'III', 'IV']:\n",
    "    print(f\">>> Dataset {dataset}:\")\n",
    "    ans = anscombe[dataset]\n",
    "    fig = least_squares_evaluation(ans['x'], ans['y'], visualize = NO_VIZ)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is only when we visualize the datasets that we realize only one of these four sets of data makes sense to model using SLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize = (10, 10))\n",
    "\n",
    "for i, dataset in enumerate(['I', 'II', 'III', 'IV']):\n",
    "    ans = anscombe[dataset]\n",
    "    axs[i//2, i%2].scatter(ans['x'], ans['y'])\n",
    "    axs[i//2, i%2].set_title(f\"Dataset {dataset}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the residuals will also shed light on the differences among these four datasets.\n",
    "\n",
    "If you remember from Data 8, \"the residual plot of a good regression shows no pattern\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['I', 'II', 'III', 'IV']:\n",
    "    print(f\">>> Dataset {dataset}:\")\n",
    "    ans = anscombe[dataset]\n",
    "    fig = least_squares_evaluation(ans['x'], ans['y'], visualize = RESID)\n",
    "    plt.show(fig)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The takeaway here is that you should always visualize your datasets before fitting any models to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Dugongs Part 1: Comparing Two Different Models, Both Fit with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dugongs = pd.read_csv(\"data/dugongs.csv\")\n",
    "data_constant = dugongs[\"Age\"]\n",
    "data_linear = dugongs[[\"Length\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Surfaces\n",
    "\n",
    "Computes constant loss surface. As a reminder, the average loss of the constant model is\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\hat{R}(\\theta_0) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\theta_0)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default') # Revert style to default mpl\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "def mse_constant(theta, data):\n",
    "    return np.mean(np.array([(y_obs - theta) ** 2 for y_obs in data]), axis=0)\n",
    "\n",
    "thetas = np.linspace(-20, 42, 1000)\n",
    "l2_loss_thetas = mse_constant(thetas, data_constant)\n",
    "\n",
    "plt.plot(thetas, l2_loss_thetas)\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'MSE')\n",
    "\n",
    "# Optimal point\n",
    "thetahat = np.mean(data_constant)\n",
    "plt.scatter([thetahat], [mse_constant(thetahat, data_constant)], s=50, label = r\"$\\hat{\\theta}_0$\")\n",
    "plt.legend()\n",
    "# plt.savefig('mse_constant_loss.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes 3D loss surface. As a reminder, the average loss for the SLR model is\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "\\hat{R}(\\theta_0, \\theta_1) = \\frac{1}{n}\\sum_{i=1}^n (y_i - (\\theta_0 + \\theta_1x))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell\n",
    "import itertools\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def mse_linear(theta_0, theta_1, data_linear):\n",
    "    data_x, data_y = data_linear.iloc[:,0], data_linear.iloc[:,1]\n",
    "    return np.mean(np.array([(y - (theta_0+theta_1*x)) ** 2 for x, y in zip(data_x, data_y)]), axis=0)\n",
    "\n",
    "theta_0_values = np.linspace(-80, 20, 80)\n",
    "theta_1_values = np.linspace(-10,30, 80)\n",
    "mse_values = [mse_linear(theta_0, theta_1, data_linear) \\\n",
    "              for theta_0, theta_1 in itertools.product(theta_0_values, theta_1_values)]\n",
    "mse_values = np.reshape(mse_values, (len(theta_0_values), len(theta_1_values)), order='F')\n",
    "\n",
    "# Optimal point\n",
    "data_x, data_y = data_linear.iloc[:,0], data_linear.iloc[:,1]\n",
    "theta_1_hat = np.corrcoef(data_x, data_y)[0, 1] * np.std(data_y) / np.std(data_x)\n",
    "theta_0_hat = np.mean(data_y) - theta_1_hat * np.mean(data_x)\n",
    "fig_lin = go.Figure(data=[go.Surface(x=theta_0_values, y=theta_1_values, z=mse_values,name='z=MSE')])\n",
    "fig_lin.add_trace(go.Scatter3d(x=[theta_0_hat], y=[theta_1_hat], \\\n",
    "                               z=[mse_linear(theta_0_hat, theta_1_hat, data_linear)],\\\n",
    "                               mode='markers', marker=dict(size=12),name='theta hat'))\n",
    "\n",
    "fig_lin.update_layout(\n",
    "    autosize=False,\n",
    "    scene = dict(\n",
    "                    xaxis_title='theta_0',\n",
    "                    yaxis_title='theta_1',\n",
    "                    zaxis_title='z=MSE'),\n",
    "                    width=500,\n",
    "                    margin=dict(r=20, b=10, l=10, t=10))\n",
    "\n",
    "fig_lin.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Least Squares Constant Model RMSE:\",\n",
    "          np.sqrt(mse_constant(thetahat, data_constant)))\n",
    "print(\"Least Squares Linear Model RMSE:  \",\n",
    "          np.sqrt(mse_linear(theta_0_hat, theta_1_hat, data_linear)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the RMSE (Root Mean Square Error):\n",
    "* Constant model error is HIGHER than linear model error\n",
    "* Linear model is BETTER than constant model (at least for this metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "This plotting code is left for your reference. We'll mainly look at the figures in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yobs = data_linear[\"Age\"]      # The true observations y\n",
    "xs = data_linear[\"Length\"]     # Needed for linear predictions\n",
    "n = len(yobs)                  # Predictions\n",
    "\n",
    "yhats_constant = [thetahat for i in range(n)]    # Not used, but food for thought\n",
    "yhats_linear = [theta_0_hat + theta_1_hat * x for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we're in a weird style state\n",
    "sns.set_theme()\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8, 1.5))\n",
    "sns.rugplot(yobs, height=0.25, lw=2) ;\n",
    "plt.axvline(thetahat, color='red', lw=4, label=r\"$\\hat{\\theta}_0$\");\n",
    "plt.legend()\n",
    "plt.yticks([])\n",
    "# plt.savefig('dugong_rug.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we're in a weird style state\n",
    "sns.set_theme()\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.scatterplot(x=xs, y=yobs)\n",
    "plt.plot(xs, yhats_linear, color='red', lw=4)\n",
    "# plt.savefig('dugong_line.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Boba Tea:  Two Constant Models, Fit to Different Losses\n",
    "\n",
    "### Exploring MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boba = np.array([20, 21, 22, 29, 33])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the $L_1$ loss for a **single** observation. We'll plot the $L_1$ loss for the first observation; since $y_1 = 20$, we'll be plotting\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "L_1(20, \\theta_0) = |20 - \\theta_0|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(10, 30, 1000)\n",
    "l1_loss_single_obvs = np.abs(boba[0] - thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default') # Revert style to default mpl\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(thetas, l1_loss_single_obvs,  'g--', );\n",
    "plt.xlabel(r'$\\theta_0$');\n",
    "plt.ylabel(r'L1 Loss for $y = 20$');\n",
    "# plt.savefig('l1_loss_single_obs.png', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_constant(theta_0, data):\n",
    "    return np.mean(np.array([np.abs(y_obs - theta_0) for y_obs in data]), axis=0)\n",
    "\n",
    "thetas = np.linspace(10, 40, 1000)\n",
    "l1_loss_thetas = mae_constant(thetas, boba)\n",
    "plt.plot(thetas, l1_loss_thetas, color = 'green', lw=3);\n",
    "plt.xlabel(r'$\\theta_0$');\n",
    "plt.ylabel(r'MAE across all data');\n",
    "# plt.savefig('l1_loss_average.png', bbox_inches = 'tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Minimizes MAE for the Constant Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we're in a weird style state\n",
    "sns.set_theme()\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "yobs = boba\n",
    "thetahat = np.median(yobs)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 1.5))\n",
    "sns.rugplot(yobs, height=0.25, lw=2) ;\n",
    "plt.scatter([thetahat], [-.1], color='green', lw=4, label=r\"$\\hat{\\theta}_0$\");\n",
    "plt.xlabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.yticks([])\n",
    "# plt.savefig('boba_rug.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Constant Models, Fit to Different Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default') # Revert style to default mpl\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "nplots = 2\n",
    "def plot_losses(data, title=None, theta_range=(10, 40)):\n",
    "    thetas = np.linspace(theta_range[0], theta_range[1], 1000)\n",
    "    l2_loss_thetas = mse_constant(thetas, data)\n",
    "    thetahat_mse = np.mean(data)\n",
    "\n",
    "    l1_loss_thetas = mae_constant(thetas, data)\n",
    "    thetahat_mae = np.median(data)\n",
    "\n",
    "    fig, axs = plt.subplots(1, nplots, figsize=(5*2+0.5, 3.5))\n",
    "    axs[0].plot(thetas, l2_loss_thetas, lw=3);\n",
    "    axs[0].scatter([thetahat_mse], [mse_constant(thetahat_mse, data)], s=100)\n",
    "    axs[0].annotate(r\"$\\hat{\\theta}_0$ = \" + f\"{thetahat_mse:.1f}\",\n",
    "                    xy=(thetahat_mse, np.average(axs[0].get_ylim())),\n",
    "                    size=20, ha='center', va='top',\n",
    "                    bbox=dict(boxstyle='round', fc='w'))\n",
    "    axs[0].set_xlabel(r'$\\theta_0$');\n",
    "    axs[0].set_ylabel(r'MSE');\n",
    "\n",
    "    axs[1].plot(thetas, l1_loss_thetas, color = 'green', lw=3);\n",
    "    axs[1].scatter([thetahat_mae], [mae_constant(thetahat_mae, data)], color='green', s=100)\n",
    "    axs[1].annotate(r\"$\\hat{\\theta}_0$ = \" + f\"{thetahat_mae:.1f}\",\n",
    "                    xy=(thetahat_mae, np.average(axs[1].get_ylim())),\n",
    "                    size=20, ha='center', va='top',\n",
    "                    bbox=dict(boxstyle='round', fc='w'))\n",
    "    axs[1].set_xlabel(r'$\\theta_0$');\n",
    "    axs[1].set_ylabel(r'MAE');\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_losses(boba)\n",
    "plt.figure(fig)\n",
    "# plt.savefig('loss_compare.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More loss comparison: Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boba_outlier = np.array([20, 21, 22, 29, 33, 1033])\n",
    "fig = plot_losses(boba_outlier, theta_range=[-10, 300])\n",
    "plt.figure(fig)\n",
    "# plt.savefig('loss_outlier.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness under Different Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boba_even = np.array([20, 21, 22, 29, 33, 35])\n",
    "fig = plot_losses(boba_even)\n",
    "plt.figure(fig)\n",
    "#plt.savefig('loss_unique.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: MAE loss curve for SLR?\n",
    "\n",
    "We saw earlier that the MSE (average L2) loss curve for Simple Linear Regression is smooth and parabolic in 3 dimensions. What does the MAE for SLR curve look like?\n",
    "\n",
    "It's **beyond the scope of this course**, but for the curious, the below cell plots the mean absolute error (average L1 loss) on the boba dataset for a simple linear regression model . I don't plot the minimum because I don't know if there's a closed form solution (you could solve it numerically with techniques you learn in lab).\n",
    "\n",
    "Since the boba dataset didn't have input, I've arbitrarily used \"temperature of that day\" as input. I made up the numbers (assume degrees Farenheit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boba_df = pd.DataFrame({\"sales\": [20, 21, 22, 29, 33],\n",
    "                        \"temps\": [40, 44, 55, 70, 85]},\n",
    "                       columns=[\"temps\", \"sales\"])\n",
    "boba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell\n",
    "import itertools\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "data_linear = boba_df\n",
    "\n",
    "def mAe_linear(theta_0, theta_1, data_linear):\n",
    "    data_x, data_y = data_linear.iloc[:,0], data_linear.iloc[:,1]\n",
    "    return np.mean(np.array([np.abs(y - (theta_0+theta_1*x)) for x, y in zip(data_x, data_y)]), axis=0)\n",
    "\n",
    "theta_0_values = np.linspace(-20, 20, 80)\n",
    "theta_1_values = np.linspace(-50, 50, 80)\n",
    "mAe_values = [mAe_linear(theta_0, theta_1, data_linear) \\\n",
    "              for theta_0, theta_1 in itertools.product(theta_0_values, theta_1_values)]\n",
    "mAe_values = np.reshape(mAe_values, (len(theta_0_values), len(theta_1_values)), order='F')\n",
    "\n",
    "# Optimal point\n",
    "fig_lin_mae = go.Figure(data=[go.Surface(x=theta_0_values, y=theta_1_values, z=mAe_values,name='z=MAE')])\n",
    "\n",
    "fig_lin_mae.update_layout(\n",
    "    autosize=False,\n",
    "    scene = dict(\n",
    "                    xaxis_title='x=theta_0',\n",
    "                    yaxis_title='y=theta_1',\n",
    "                    zaxis_title='z=MAE'),\n",
    "                    width=500,\n",
    "                    margin=dict(r=20, b=10, l=10, t=10))\n",
    "fig_lin_mae.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now observe the difference between the MAE loss and the MSE loss for one application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell\n",
    "\n",
    "def mSe_linear(theta_0, theta_1, data_linear):\n",
    "    data_x, data_y = data_linear.iloc[:,0], data_linear.iloc[:,1]\n",
    "    return np.mean(np.array([(y - (theta_0+theta_1*x))**2 for x, y in zip(data_x, data_y)]), axis=0)\n",
    "\n",
    "theta_0_values = np.linspace(-20, 20, 80)\n",
    "theta_1_values = np.linspace(-50, 50, 80)\n",
    "mSe_values = [mSe_linear(theta_0, theta_1, data_linear) \\\n",
    "              for theta_0, theta_1 in itertools.product(theta_0_values, theta_1_values)]\n",
    "mSe_values = np.reshape(mSe_values, (len(theta_0_values), len(theta_1_values)), order='F')\n",
    "\n",
    "# Optimal point\n",
    "fig_lin_mae = go.Figure(data=[go.Surface(x=theta_0_values, y=theta_1_values, z=mSe_values,name='z=MSE')])\n",
    "\n",
    "fig_lin_mae.update_layout(\n",
    "    autosize=False,\n",
    "    scene = dict(\n",
    "                    xaxis_title='x=theta_0',\n",
    "                    yaxis_title='y=theta_1',\n",
    "                    zaxis_title='z=MSE'),\n",
    "                    width=500,\n",
    "                    margin=dict(r=20, b=10, l=10, t=10))\n",
    "fig_lin_mae.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dugongs Part 2\n",
    "\n",
    "### Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dugongs = pd.read_csv(\"data/dugongs.csv\")\n",
    "dugongs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yobs = dugongs[\"Age\"]      # The true observations y\n",
    "xs = dugongs[\"Length\"]     # Needed for linear predictions\n",
    "\n",
    "theta_1_hat = np.corrcoef(xs, yobs)[0, 1] * np.std(yobs) / np.std(xs)\n",
    "theta_0_hat = np.mean(yobs) - theta_1_hat * np.mean(xs)\n",
    "yhats_linear = theta_0_hat + theta_1_hat * xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(xs, yobs)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient is pretty high...but there's an issue.\n",
    "\n",
    "Let's first plot the Dugong linear fit again. It doesn't look so bad if we see it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we're in a weird style state\n",
    "sns.set_theme()\n",
    "adjust_fontsize(size=16)\n",
    "%matplotlib inline\n",
    "\n",
    "sns.scatterplot(x=xs, y=yobs)\n",
    "plt.plot(xs, yhats_linear, color='red', lw=4)\n",
    "# plt.savefig('dugong_line.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further inspect by plotting residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = yobs - yhats_linear\n",
    "\n",
    "sns.scatterplot(x=xs, y=residuals, color='red', lw=4)\n",
    "plt.axhline(0, color='black')\n",
    "plt.ylabel(r\"$y - \\hat{y}$\")\n",
    "# plt.savefig('dugong_residuals.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could fit a line to the linear model that relates $ z = \\log(y)$ to $x$:\n",
    "\n",
    "$$ \n",
    "\\Large\n",
    "\\hat{z} = \\theta_0 + \\theta_1 x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dugongs[\"Log(Age)\"] = np.log(dugongs[\"Age\"])\n",
    "zobs = dugongs[\"Log(Age)\"]      # The LOG of true observations y\n",
    "xs = dugongs[\"Length\"]     # Needed for linear predictions\n",
    "\n",
    "ztheta_1_hat = np.corrcoef(xs, zobs)[0, 1] * np.std(zobs) / np.std(xs)\n",
    "ztheta_0_hat = np.mean(zobs) - ztheta_1_hat * np.mean(xs)\n",
    "zhats_linear = ztheta_0_hat + ztheta_1_hat * xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=xs, y=zobs, color='green')\n",
    "plt.plot(xs, zhats_linear, lw=4)\n",
    "# plt.savefig('dugong_zline.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zresiduals = zobs - zhats_linear\n",
    "\n",
    "sns.scatterplot(x=xs, y=zresiduals, color='red', lw=4)\n",
    "plt.axhline(0, color='black')\n",
    "plt.ylabel(r\"$z - \\hat{z}$\")\n",
    "# plt.savefig('dugong_zresiduals.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map back to the original coordinates\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\hat{z} &= \\theta_0 + \\theta_1 x\\\\\n",
    "\\widehat{\\log(y)}&= \\theta_0 + \\theta_1 x\\\\\n",
    "e^{\\widehat{\\log(y)}}&= e^{\\theta_0 + \\theta_1 x}\\\\\n",
    "\\hat{y}&=e^{\\theta_0 + \\theta_1 x}\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.exp(zhats_linear)\n",
    "sns.scatterplot(x=xs, y=yobs)\n",
    "plt.plot(xs, ypred, color='green', lw=4)\n",
    "# plt.savefig('dugong_curve.png', bbox_inches = 'tight');\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
