{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc255dd2-0ee4-4308-b2cc-173178ea709c",
   "metadata": {},
   "source": [
    "# Lecture 14 – Data 100, Spring 2024\n",
    "\n",
    "Data 100, Spring 2024\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/sp24/acks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.mode.chained_assignment = None \n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81168b67-b5ff-4005-8c06-608e07f2c7b2",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the process of applying **feature functions** to generate new features for use in modeling. In this lecture, we will discuss:\n",
    "* One-hot encoding\n",
    "* Polynomial features\n",
    "\n",
    "To do so, we'll work with our familiar `tips` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a66496",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fefbb1-3125-42f5-ab5d-4dfd0c36e961",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a feature engineering technique to generate numeric feature from categorical data. For example, we can use one-hot encoding to incorporate the day of the week as an input into a regression model.\n",
    "\n",
    "![](ohe.png)\n",
    "\n",
    "Suppose we want to use a design matrix of three features – the `total_bill`, `size`, and `day` – to predict the tip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = tips[[\"total_bill\", \"size\", \"day\"]]\n",
    "y = tips[\"tip\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d683b1-a7cf-45ed-bf65-1b4ebb66da50",
   "metadata": {},
   "source": [
    "Because `day` is non-numeric, we will apply one-hot encoding before fitting a model.\n",
    "\n",
    "The `OneHotEncoder` class of `sklearn` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.get_feature_names_out)) offers a quick way to perform one-hot encoding. You will explore its use in detail in Lab 7. For now, recognize that we follow a very similar workflow to when we were working with the `LinearRegression` class: we initialize a `OneHotEncoder` object, fit it to our data, then use `.transform` to apply the fitted encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc958780-bfb8-45c7-a33a-27a9b6d276d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize a OneHotEncoder object\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder\n",
    "ohe.fit(tips[[\"day\"]])\n",
    "\n",
    "# Use the encoder to transform the raw \"day\" feature\n",
    "encoded_day = ohe.transform(tips[[\"day\"]]).toarray()\n",
    "encoded_day_df = pd.DataFrame(encoded_day, columns=ohe.get_feature_names_out())\n",
    "\n",
    "encoded_day_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68105ccb-6f42-4b38-9b76-6752b7c7df72",
   "metadata": {},
   "source": [
    "The `OneHotEncoder` has converted the categorical `day` feature into four numeric features! Recall that the `tips` dataset only included data for Thursday through Sunday, which is why only four days of the week appear. \n",
    "\n",
    "Let's join this one-hot encoding to the original data to form our featurized design matrix. We drop the original `day` column so our design matrix only includes numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea867e6b-742a-4627-95a2-f0f80d1a0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_raw.join(encoded_day_df).drop(columns=\"day\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6dc523-b40c-45f7-8bf4-c74bd4e6f375",
   "metadata": {},
   "source": [
    "Now, we can use `sklearn`'s `LinearRegression` class to fit a model to this design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8bcc7e-642b-4cf7-bb7b-5d74adc24f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ohe_model = LinearRegression(fit_intercept=False) # Tell sklearn to not add an additional bias column. Why?\n",
    "ohe_model.fit(X, y)\n",
    "\n",
    "pd.DataFrame({\"Feature\":X.columns, \"Model Coefficient\":ohe_model.coef_}).set_index(\"Feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f812912-becc-4475-b33e-add6e2b3a8c4",
   "metadata": {},
   "source": [
    "### Bonus: Advanced Feature Engineering Pipelines in Scikit Learn\n",
    "You can use the scikit learn `Pipeline` class to define sophisticated feature engineering pipelines that combine different transformations that are all fit at once.\n",
    "\n",
    "A pipeline is a sequence of transformations and models that are composed to form a new model. It is implemented as a list of  (\"name\", model/transformation) tuples.  Here we have a feature engineering stage followed by a linear model. The feature engineering stage uses a `ColumnTransformer` that applies named transformations to selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafdf96d-96ae-440f-b820-2ec649fc01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"Feature Engineering\", \n",
    "     ColumnTransformer(  \n",
    "         [(\"OHE\", OneHotEncoder(), [\"day\"]),\n",
    "         (\"Polynomial\", PolynomialFeatures(4), ['total_bill'])], \n",
    "        remainder=\"passthrough\")), \n",
    "    (\"Linear Model\", LinearRegression()) # Linear Model\n",
    "]) \n",
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26117cc6-5564-47c2-9953-71a42533f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(X_raw, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863159a-d922-42f9-8637-5ba9e6968119",
   "metadata": {},
   "source": [
    "All the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e32f0-38a8-4306-adb7-46b82094e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl['Feature Engineering'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fcea1-a1af-46db-a757-1e5ce9abdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips[\"predicted\"] = pl.predict(X_raw)\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5356ce-c42c-4955-b803-01f39d7ee680",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "**Return to Lecture**\n",
    "\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0615660-3122-4181-aeb9-ef886cea19ea",
   "metadata": {},
   "source": [
    "### Polynomial Features\n",
    "\n",
    "Consider the `vehicles` dataset, which includes information about cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716c468-74e5-4960-864b-40f2e8af5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = sns.load_dataset(\"mpg\").dropna().rename(columns = {\"horsepower\": \"hp\"}).sort_values(\"hp\")\n",
    "vehicles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27378d6-516e-417a-b80a-0282ea4243df",
   "metadata": {},
   "source": [
    "Suppose we want to use the `hp` (horsepower) of a car to predict its `mpg` (gas mileage in miles per gallon). If we visualize the relationship between these two variables, we see a non-linear curvature. Fitting a linear model to these variables results in a high (poor) value of RMSE. \n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 (\\text{hp})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33629843-f23d-4624-accf-a9b6af482d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vehicles[[\"hp\"]]\n",
    "y = vehicles[\"mpg\"]\n",
    "\n",
    "hp_model = LinearRegression()\n",
    "hp_model.fit(X, y)\n",
    "hp_model_predictions = hp_model.predict(X)\n",
    "\n",
    "print(f\"MSE of model with (hp) feature: {np.mean((y-hp_model_predictions)**2)}\")\n",
    "\n",
    "fig = px.scatter(vehicles, x=\"hp\", y=\"mpg\", width=800, height=600)\n",
    "fig.add_trace(go.Scatter(x=vehicles[\"hp\"], y=hp_model_predictions,\n",
    "                         mode=\"lines\", name=\"Linear Prediction\"))\n",
    "# sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\")\n",
    "# plt.plot(vehicles[\"hp\"], hp_model_predictions, c=\"tab:red\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249691a7-ea35-493c-ad04-f224e61f50c4",
   "metadata": {},
   "source": [
    "To capture the non-linear relationship between the variables, we can introduce a non-linear feature: `hp` squared. Our new model is:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 (\\text{hp}) + \\theta_2 (\\text{hp}^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f79d3-df91-408e-b78d-3ba61a7a6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vehicles[[\"hp\"]]\n",
    "X.loc[:, \"hp^2\"] = vehicles[\"hp\"]**2\n",
    "\n",
    "hp2_model = LinearRegression()\n",
    "hp2_model.fit(X, y)\n",
    "hp2_model_predictions = hp2_model.predict(X)\n",
    "\n",
    "\n",
    "print(f\"MSE of model with (hp^2) feature: {np.mean((y-hp2_model_predictions)**2)}\")\n",
    "\n",
    "fig = px.scatter(vehicles, x=\"hp\", y=\"mpg\", width=800, height=600)\n",
    "fig.add_trace(go.Scatter(x=vehicles[\"hp\"], y=hp2_model_predictions,\n",
    "                         mode=\"lines\", name=\"Quadratic Prediction\"))\n",
    "# sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\")\n",
    "# plt.plot(vehicles[\"hp\"], hp2_model_predictions, c=\"tab:red\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337bad5-873a-48ca-aa70-ba2467b60619",
   "metadata": {},
   "source": [
    "What if we take things further and add even *more* polynomial features?\n",
    "\n",
    "The cell below fits models of increasing complexity and computes their MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032da0c-757a-4472-940c-e72718bc514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predictions, observations):\n",
    "    return np.mean((observations - predictions)**2)\n",
    "\n",
    "# Add hp^3 and hp^4 as features to the data\n",
    "X[\"hp^3\"] = vehicles[\"hp\"]**3\n",
    "X[\"hp^4\"] = vehicles[\"hp\"]**4\n",
    "\n",
    "# Fit a model with order 3\n",
    "hp3_model = LinearRegression()\n",
    "hp3_model.fit(X[[\"hp\", \"hp^2\", \"hp^3\"]], vehicles[\"mpg\"])\n",
    "hp3_model_predictions = hp3_model.predict(X[[\"hp\", \"hp^2\", \"hp^3\"]])\n",
    "\n",
    "# Fit a model with order 4\n",
    "hp4_model = LinearRegression()\n",
    "hp4_model.fit(X[[\"hp\", \"hp^2\", \"hp^3\", \"hp^4\"]], vehicles[\"mpg\"])\n",
    "hp4_model_predictions = hp4_model.predict(X[[\"hp\", \"hp^2\", \"hp^3\", \"hp^4\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41811ae-72f0-4fb7-9f8c-8a57960386fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(vehicles, x=\"hp\", y=\"mpg\", width=800, height=600)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vehicles[\"hp\"], y=hp_model_predictions, mode=\"lines\", \n",
    "    name=f\"HP MSE={np.round(mse(vehicles['mpg'], hp_model_predictions),3)}\"))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vehicles[\"hp\"], y=hp2_model_predictions, mode=\"lines\", \n",
    "    name=f\"HP^2 MSE={np.round(mse(vehicles['mpg'], hp2_model_predictions),3)}\"))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vehicles[\"hp\"], y=hp3_model_predictions, mode=\"lines\", \n",
    "    name=f\"HP^3 MSE={np.round(mse(vehicles['mpg'], hp3_model_predictions),3)}\"))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=vehicles[\"hp\"], y=hp4_model_predictions, mode=\"lines\", \n",
    "    name=f\"HP^4 MSE={np.round(mse(vehicles['mpg'], hp4_model_predictions),3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e18e62-17fb-4530-a4a4-dda3a999b03d",
   "metadata": {},
   "source": [
    "Visualizing differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791063a6-24ef-4f88-8e65-e457112a9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the models' predictions\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "predictions_dict = {0:hp2_model_predictions, 1:hp3_model_predictions, 2:hp4_model_predictions}\n",
    "\n",
    "for i in predictions_dict:\n",
    "    ax[i].scatter(vehicles[\"hp\"], vehicles[\"mpg\"], edgecolor=\"white\", lw=0.5)\n",
    "    ax[i].plot(vehicles[\"hp\"], predictions_dict[i], \"tab:red\")\n",
    "    ax[i].set_title(f\"Model with order {i+2}\")\n",
    "    ax[i].set_xlabel(\"hp\")\n",
    "    ax[i].set_ylabel(\"mpg\")\n",
    "    ax[i].annotate(f\"MSE: {np.round(mse(vehicles['mpg'], predictions_dict[i]), 3)}\", (120, 40))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e266b33f-37c6-4d39-b19c-f18fc576b19a",
   "metadata": {},
   "source": [
    "## Complexity and Overfitting\n",
    "\n",
    "What we saw above was the phenomenon of **model complexity** – as we add additional features to the design matrix, the model becomes increasingly *complex*. Models with higher complexity have lower values of training error. Intuitively, this makes sense: with more features at its disposal, the model can match the observations in the trainining data more and more closely. \n",
    "\n",
    "We can run an experiment to see this in action. In the cell below, we fit many models of progressively higher complexity, then plot the MSE of predictions on the training set. The code used (specifically, the `Pipeline` and `PolynomialFeatures` functions of `sklearn`) is out of scope.\n",
    "\n",
    "The **order** of a polynomial model is the highest power of any term in the model. An order 0 model takes the form $\\hat{y} = \\theta_0$, while an order 4 model takes the form $\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3 + \\theta_4 x^4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c06c3-f3bb-452f-acef-6df2661affbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def fit_model_dataset(degree, dataset):\n",
    "    pipelined_model = Pipeline([\n",
    "            ('polynomial_transformation', PolynomialFeatures(degree)),\n",
    "            ('linear_regression', LinearRegression())    \n",
    "        ])\n",
    "\n",
    "    pipelined_model.fit(dataset[[\"hp\"]], dataset[\"mpg\"])\n",
    "    return mse(dataset['mpg'], pipelined_model.predict(dataset[[\"hp\"]]))\n",
    "\n",
    "errors = [fit_model_dataset(degree, vehicles) for degree in range(0, 8)]\n",
    "MSEs_and_k = pd.DataFrame({\"k\": range(0, 8), \"MSE\": errors})\n",
    "\n",
    "plt.plot(range(0, 8), errors)\n",
    "plt.xlabel(\"Model Complexity (degree of polynomial)\")\n",
    "plt.ylabel(\"Training MSE\");\n",
    "\n",
    "def plot_degree_k_model(k, MSEs_and_k, axs):\n",
    "    pipelined_model = Pipeline([\n",
    "        ('poly_transform', PolynomialFeatures(degree = k)),\n",
    "        ('regression', LinearRegression(fit_intercept = True))    \n",
    "    ])\n",
    "    pipelined_model.fit(vehicles[[\"hp\"]], vehicles[\"mpg\"])\n",
    "    \n",
    "    row = k // 4\n",
    "    col = k % 4\n",
    "    ax = axs[row, col]\n",
    "    \n",
    "    sns.scatterplot(data=vehicles, x='hp', y='mpg', ax=ax)\n",
    "    \n",
    "    x_range = np.linspace(45, 210, 100).reshape(-1, 1)\n",
    "    ax.plot(x_range, pipelined_model.predict(pd.DataFrame(x_range, columns=['hp'])), c='tab:red', linewidth=2)\n",
    "    \n",
    "    ax.set_ylim((0, 50))\n",
    "    mse_str = f\"MSE: {MSEs_and_k.loc[k, 'MSE']:.4}\\nDegree: {k}\"\n",
    "    ax.text(130, 35, mse_str, dict(size=14))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6), dpi=150)\n",
    "axs = fig.subplots(nrows=2, ncols=4)\n",
    "\n",
    "for k in range(8):\n",
    "    plot_degree_k_model(k, MSEs_and_k, axs)\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0197e1e3-681c-4192-8cd1-b7c97c936cbe",
   "metadata": {},
   "source": [
    "As the model increases in polynomial degree (that is, it increases in complexity), the training MSE decreases, plateauing at roughly ~18.\n",
    "\n",
    "In fact, it is a mathematical fact that if we create a polynomial model with degree $n-1$, we can *perfectly* model a set of $n$ points. For example, a set of 5 points can be perfectly modeled by a degree 4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb0a9a-7052-4dcc-83a3-3b72546688fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    points = 3*np.random.uniform(size=(5, 2))\n",
    "\n",
    "    polynomial_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(4)),\n",
    "                ('linear_regression', LinearRegression())    \n",
    "            ])\n",
    "\n",
    "    polynomial_model.fit(points[:, [0]], points[:, 1])\n",
    "\n",
    "    ax[i].scatter(points[:, 0], points[:, 1])\n",
    "\n",
    "    xs = np.linspace(0, 3)\n",
    "    ax[i].plot(xs, polynomial_model.predict(xs[:, np.newaxis]), c=\"tab:red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91667111-8fef-4fed-9c23-6f204796737a",
   "metadata": {},
   "source": [
    "You may be tempted to always design models with high polynomial degree – after all, we know that we could theoretically achieve perfect predictions by creating a model with enough polynomial features. \n",
    "\n",
    "It turns out that the examples we looked at above represent a somewhat artificial scenario: we trained our model on all the data we had available, then used the model to make predictions on this very same dataset. A more realistic situation is when we wish to apply our model on unseen data – that is, datapoints that it did not encounter during the model fitting process. \n",
    "\n",
    "Suppose we obtain a random sample of 6 datapoints from our population of vehicle data. We want to train a model on these 6 points and use it to make predictions on unseen data (perhaps cars for which we don't already know the true `mpg`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56a90f-6a45-4383-9b91-a1527744f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "sample_6 = vehicles.sample(6)\n",
    "\n",
    "sns.scatterplot(data=sample_6, x=\"hp\", y=\"mpg\")\n",
    "plt.ylim(-35, 50)\n",
    "plt.xlim(0, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3200ca-4f9a-4bd3-a63b-c2152e38e86d",
   "metadata": {},
   "source": [
    "If we design a model with polynomial degree 5, we can make perfect predictions on this sample of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf0537-6172-44f1-9685-8026576313ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_5_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(5)),\n",
    "                ('linear_regression', LinearRegression())    \n",
    "            ])\n",
    "\n",
    "degree_5_model.fit(sample_6[[\"hp\"]], sample_6[\"mpg\"])\n",
    "xs = np.linspace(0, 250, 1000)\n",
    "degree_5_model_predictions = degree_5_model.predict(xs[:, np.newaxis])\n",
    "\n",
    "plt.plot(xs, degree_5_model_predictions, c=\"tab:red\")\n",
    "sns.scatterplot(data=sample_6, x=\"hp\", y=\"mpg\", s=50)\n",
    "plt.ylim(-35, 50)\n",
    "plt.xlim(0, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d19c1df-fb2d-4d5f-a623-10cc7c241976",
   "metadata": {},
   "source": [
    "However, when we reapply this fitted model to the full population of data, it fails to capture the major trends of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b9bb6-5c12-47c4-85d9-7c7fa6b61ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xs, degree_5_model_predictions, c=\"tab:red\")\n",
    "sns.scatterplot(data=vehicles, x=\"hp\", y=\"mpg\", s=50)\n",
    "plt.ylim(-35, 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59caca6-18a7-4568-b4ec-e5f4ee02a505",
   "metadata": {},
   "source": [
    "The model has **overfit** to the data used to train it. It has essentially \"memorized\" the six datapoints used during model fitting, and does not generalize well to new data. \n",
    "\n",
    "Complex models tend to be more sensitive to the data used to train them. The **variance** of a model refers to its tendency to vary depending on the training data used during model fitting. It turns out that our degree-5 model has very high model variance. If we randomly sample new sets of datapoints to use in training, the model varies erratically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc6d82-afb9-4a4a-bbc2-73d0b8904400",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, dpi=200, figsize=(12, 3))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    sample = vehicles.sample(6)\n",
    "\n",
    "    polynomial_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(5)),\n",
    "                ('linear_regression', LinearRegression())    \n",
    "            ])\n",
    "\n",
    "    polynomial_model.fit(sample[[\"hp\"]], sample[\"mpg\"])\n",
    "\n",
    "    ax[i].scatter(sample[[\"hp\"]], sample[\"mpg\"])\n",
    "\n",
    "    xs = np.linspace(50, 210, 1000)\n",
    "    ax[i].plot(xs, polynomial_model.predict(xs[:, np.newaxis]), c=\"tab:red\")\n",
    "    ax[i].set_ylim(-80, 100)\n",
    "    ax[i].set_xlabel(\"hp\")\n",
    "    ax[i].set_ylabel(\"mpg\")\n",
    "    ax[i].set_title(f\"Resample #{i+1}\")\n",
    "    \n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f36b32-eab0-4982-8f88-d988b71f3604",
   "metadata": {},
   "source": [
    "Visualizing differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fed5aa-adac-4990-be53-874486ac3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "\n",
    "fig = px.scatter(vehicles, x=\"hp\", y=\"mpg\", width=800, height=600)\n",
    "\n",
    "for i in range(0, 3):\n",
    "    sample = vehicles.sample(6).sort_values(\"hp\")\n",
    "    \n",
    "    polynomial_model = Pipeline([\n",
    "                ('polynomial_transformation', PolynomialFeatures(5)),\n",
    "                ('linear_regression', LinearRegression())    \n",
    "            ])\n",
    "\n",
    "    polynomial_model.fit(sample[[\"hp\"]], sample[\"mpg\"])\n",
    "    \n",
    "    name = f\"Sample {i+1}\"\n",
    "    fig.add_trace(go.Scatter(x=sample[\"hp\"], y=sample[\"mpg\"], \n",
    "        name=name, legendgroup=name, mode=\"markers\", showlegend=False,\n",
    "        marker_size=16,\n",
    "        marker_color=px.colors.qualitative.Plotly[i+1]))\n",
    "    xs = np.linspace(10, 250, 1000)\n",
    "    fig.add_trace(go.Scatter(x=xs, \n",
    "        y=polynomial_model.predict(xs[:, np.newaxis]),\n",
    "        name=name, legendgroup=name, mode=\"lines\",\n",
    "        line_color=px.colors.qualitative.Plotly[i+1]))\n",
    "\n",
    "fig.update_yaxes(range=[0, 50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
